{"id": "bugfix001", "buggy_file": "data/raw_new2/bugfix001_buggy.py", "fixed_file": "data/raw_new2/bugfix001_fixed.py", "meta": {"repo": "GantMan/nsfw_model", "file": "nsfw_detector/predict.py", "buggy_sha": "7014aba25b3ec24be442394a7b049e1b058a99f4", "fixed_sha": "fbd8fdc632e66352f83e54fd3ff594824d5717cc", "commit_message": "Merge pull request #104 from OttomanZ/master\n\nBug Fixes", "changes": 3, "patch": "@@ -49,12 +49,11 @@ def load_images(image_paths, image_size, verbose=True):\n     \n     return np.asarray(loaded_images), loaded_image_paths\n \n-\n def load_model(model_path):\n     if model_path is None or not exists(model_path):\n     \traise ValueError(\"saved_model_path must be the valid directory of a saved model to load.\")\n     \n-    model = tf.keras.models.load_model(model_path, custom_objects={'KerasLayer': hub.KerasLayer})\n+    model = tf.keras.models.load_model(model_path, custom_objects={'KerasLayer': hub.KerasLayer},compile=False)\n     return model\n \n ", "syntax_error": false, "llm_analysis": {"is_bug": true, "is_runtime_error": true, "short_label": "missing_compile_false_for_load_model", "bug_description": "在加载TensorFlow模型时，buggy_code版本缺少compile=False参数，这会导致当模型包含自定义损失函数或优化器时，程序在加载模型时抛出异常。fixed版本通过添加compile=False参数来修复这个问题，确保模型能够正确加载而无需重新编译。"}}}
{"id": "bugfix002", "buggy_file": "data/raw_new2/bugfix002_buggy.py", "fixed_file": "data/raw_new2/bugfix002_fixed.py", "meta": {"repo": "GantMan/nsfw_model", "file": "setup.py", "buggy_sha": "03a69c4aa582e25ea137d706d99dccd14b79bf3f", "fixed_sha": "4fd58f6e4097cb95e0760a5da8fce8cbec803537", "commit_message": "Merge pull request #58 from ezavesky/fix/mobilenet\n\nFix/mobilenet", "changes": 23, "patch": "@@ -18,12 +18,7 @@\n EMAIL = 'gantman@gmail.com'\n AUTHOR = 'Gant Laborde'\n REQUIRES_PYTHON = '>=3.5.0'\n-VERSION = '1.0.0'\n-\n-# What packages are required for this module to be executed?\n-REQUIRED = [\n-    'keras'\n-]\n+VERSION = '1.1.0'\n \n # What packages are optional?\n EXTRAS = {\n@@ -37,6 +32,16 @@\n \n here = os.path.abspath(os.path.dirname(__file__))\n \n+# Import the requirements.\n+REQUIRED = []\n+try:\n+    with io.open(os.path.join(here, 'requirements.txt'), encoding='utf-8') as f:\n+        for line_req in f:\n+            if line_req[0] != '#':\n+                REQUIRED.append(line_req.strip())\n+except FileNotFoundError:\n+    REQUIRED = []\n+\n # Import the README and use it as the long-description.\n # Note: this will only work if 'README.md' is present in your MANIFEST.in file!\n try:\n@@ -125,6 +130,10 @@ def run(self):\n     ],\n     # $ setup.py publish support.\n     cmdclass={\n-        'upload': UploadCommand,\n+        'upload': UploadCommand\n     },\n+    entry_points=\"\"\"\n+        [console_scripts]\n+        nsfw-predict=nsfw_detector.predict:main\n+    \"\"\"\n )", "syntax_error": false, "llm_analysis": {"is_bug": true, "is_runtime_error": true, "short_label": "missing_dependencies_in_setup", "bug_description": "buggy_code 硬编码了依赖包列表，只包含 'keras'，但实际项目需要更多依赖包（在 requirements.txt 中定义）。这会导致安装时缺少必要的依赖包，运行时可能因缺少模块而抛出 ImportError。fixed_code 改为从 requirements.txt 文件读取依赖包列表，确保所有必需的依赖都能正确安装。"}}}
{"id": "bugfix003", "buggy_file": "data/raw_new2/bugfix003_buggy.py", "fixed_file": "data/raw_new2/bugfix003_fixed.py", "meta": {"repo": "mitsuhiko/pipsi", "file": "get-pipsi.py", "buggy_sha": "136842278e77df81dd06d45966eb10625f9a373c", "fixed_sha": "52de3977899a085438bb75e864555871c8c2b637", "commit_message": "Merge pull request #136 from alprs/fix-py3-virtualenv\n\nmake sure pipsi venv gets created with correct python version", "changes": 7, "patch": "@@ -82,7 +82,12 @@ def _cleanup():\n         except (OSError, IOError):\n             pass\n \n-    if call([sys.executable, '-m', venv_pkg, venv]) != 0:\n+    venv_cmd = [sys.executable, '-m', venv_pkg]\n+    if venv_pkg == 'virtualenv':\n+        venv_cmd += ['-p', sys.executable]\n+    venv_cmd += [venv]\n+\n+    if call(venv_cmd) != 0:\n         _cleanup()\n         fail('Could not create virtualenv for pipsi :(')\n ", "syntax_error": false, "llm_analysis": {"is_bug": true, "is_runtime_error": true, "short_label": "missing_python_interpreter_for_virtualenv", "bug_description": "在buggy_code中，当使用virtualenv包创建虚拟环境时，没有指定Python解释器路径，这可能导致在某些系统上virtualenv使用错误的Python解释器。fixed_code通过添加'-p'参数明确指定使用当前Python解释器(sys.executable)来创建虚拟环境，确保一致性。"}}}
{"id": "bugfix004", "buggy_file": "data/raw_new2/bugfix004_buggy.py", "fixed_file": "data/raw_new2/bugfix004_fixed.py", "meta": {"repo": "nicoboss/nsz", "file": "nsz/BlockDecompressorReader.py", "buggy_sha": "307ea5f09b15928664f1af479cab36b1569d8636", "fixed_sha": "afdcb3b6567238d29b64738ce436a0ce0c3d63f1", "commit_message": "Merge pull request #211 from ITotalJustice/master\n\nfix ncz block decompression when BlockSize is a multiple of decompressedSize\nThis fixes #210", "changes": 5, "patch": "@@ -28,7 +28,10 @@ def __decompressBlock(self, blockID):\n \t\tif blockID >= len(self.CompressedBlockOffsetList) - 1:\n \t\t\tif blockID >= len(self.CompressedBlockOffsetList):\n \t\t\t\traise EOFError(\"BlockID exceeds the amounts of compressed blocks in that file!\")\n-\t\t\tdecompressedBlockSize = self.BlockHeader.decompressedSize % self.BlockSize\n+\t\t\tremainder = self.BlockHeader.decompressedSize % self.BlockSize\n+\t\t\t# https://github.com/nicoboss/nsz/issues/210\n+\t\t\tif remainder > 0:\n+\t\t\t\tdecompressedBlockSize = remainder\n \t\tself.nspf.seek(self.CompressedBlockOffsetList[blockID])\n \t\tif self.CompressedBlockSizeList[blockID] < decompressedBlockSize:\n \t\t\tself.CurrentBlock = ZstdDecompressor().decompress(self.nspf.read(self.CompressedBlockSizeList[blockID]))", "syntax_error": false, "llm_analysis": {"is_bug": true, "is_runtime_error": true, "short_label": "zero_remainder_decompression_error", "bug_description": "在最后一个数据块的处理中，当解压缩大小正好是块大小的整数倍时（余数为0），buggy_code会将decompressedBlockSize设置为0，导致后续读取0字节数据或解压缩失败。fixed版本添加了余数检查，只有当余数大于0时才使用余数作为解压缩大小，否则保持默认的块大小。"}}}
{"id": "bugfix005", "buggy_file": "data/raw_new2/bugfix005_buggy.py", "fixed_file": "data/raw_new2/bugfix005_fixed.py", "meta": {"repo": "MatthewKuKanich/FindMyFlipper", "file": "AirTagGeneration/RequestReport&Map.py", "buggy_sha": "e5a678e33dd17af46bb63050be4c9d10e17f6252", "fixed_sha": "49f89d246c93b82108dfd2d5868ffcd2d1a1e57b", "commit_message": "Fix decryption issue  (#109)\n\n* Fix decryption issue #100\n\n* Refactor to use Decryptor class\n\nThanks ArtemKiyashko!", "changes": 28, "patch": "@@ -3,27 +3,16 @@\n import base64\n import datetime\n import glob\n-import hashlib\n import json\n import os\n import sqlite3\n import struct\n import requests\n import subprocess\n-from cryptography.hazmat.backends import default_backend\n-from cryptography.hazmat.primitives.asymmetric import ec\n-from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\n+from Decryptor import Decryptor\n from cores.pypush_gsa_icloud import icloud_login_mobileme, generate_anisette_headers\n script_name = 'advanced_map_loc.py'\n \n-def sha256(data):\n-    digest = hashlib.new(\"sha256\")\n-    digest.update(data)\n-    return digest.digest()\n-\n-def decrypt(enc_data, algorithm_dkey, mode):\n-    decryptor = Cipher(algorithm_dkey, mode, default_backend()).decryptor()\n-    return decryptor.update(enc_data) + decryptor.finalize()\n \n def decode_tag(data):\n     latitude = struct.unpack(\">i\", data[0:4])[0] / 10000000.0\n@@ -101,19 +90,8 @@ def getAuth(regenerate=False, second_factor='sms'):\n             timestamp = int.from_bytes(data[0:4], 'big') + 978307200\n \n             if timestamp >= startdate:\n-                # check if NULL bytes are present in the data\n-                adj = len(data) - 88\n-\n-                # If so slice the data accordingly | Thanks, @c4pitalSteez!\n-                eph_key = ec.EllipticCurvePublicKey.from_encoded_point(ec.SECP224R1(), data[5+adj:62+adj])\n-                shared_key = ec.derive_private_key(priv, ec.SECP224R1(), default_backend()).exchange(ec.ECDH(), eph_key)\n-                symmetric_key = sha256(shared_key + b'\\x00\\x00\\x00\\x01' + data[5+adj:62+adj])\n-                decryption_key = symmetric_key[:16]\n-                iv = symmetric_key[16:]\n-                enc_data = data[62+adj:72+adj]\n-                auth_tag = data[72+adj:]\n-\n-                decrypted = decrypt(enc_data, algorithms.AES(decryption_key), modes.GCM(iv, auth_tag))\n+                decryptor = Decryptor(data, priv)\n+                decrypted = decryptor.Decrypt()\n                 tag = decode_tag(decrypted)\n                 tag['timestamp'] = timestamp\n                 tag['isodatetime'] = datetime.datetime.fromtimestamp(timestamp).isoformat()", "syntax_error": false, "llm_analysis": {"is_bug": true, "is_runtime_error": true, "short_label": "incorrect_payload_decryption_logic", "bug_description": "buggy_code 中的解密逻辑存在错误，它假设所有 payload 数据长度固定为 88 字节，通过 'adj = len(data) - 88' 计算偏移量。当实际 payload 长度不等于 88 时，会导致切片索引错误，引发运行时异常。fixed_code 通过引入 Decryptor 类封装正确的解密逻辑，避免了硬编码的长度假设，确保在各种 payload 长度下都能正确解密。"}}}
{"id": "bugfix006", "buggy_file": "data/raw_new2/bugfix006_buggy.py", "fixed_file": "data/raw_new2/bugfix006_fixed.py", "meta": {"repo": "MatthewKuKanich/FindMyFlipper", "file": "AirTagGeneration/request_reports.py", "buggy_sha": "e5a678e33dd17af46bb63050be4c9d10e17f6252", "fixed_sha": "49f89d246c93b82108dfd2d5868ffcd2d1a1e57b", "commit_message": "Fix decryption issue  (#109)\n\n* Fix decryption issue #100\n\n* Refactor to use Decryptor class\n\nThanks ArtemKiyashko!", "changes": 34, "patch": "@@ -3,31 +3,15 @@\n import base64\n import datetime\n import glob\n-import hashlib\n import json\n import os\n import sqlite3\n import struct\n-\n import requests\n-from cryptography.hazmat.backends import default_backend\n-from cryptography.hazmat.primitives.asymmetric import ec\n-from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\n-\n+from Decryptor import Decryptor\n from cores.pypush_gsa_icloud import icloud_login_mobileme, generate_anisette_headers\n \n \n-def sha256(data):\n-    digest = hashlib.new(\"sha256\")\n-    digest.update(data)\n-    return digest.digest()\n-\n-\n-def decrypt(enc_data, algorithm_dkey, mode):\n-    decryptor = Cipher(algorithm_dkey, mode, default_backend()).decryptor()\n-    return decryptor.update(enc_data) + decryptor.finalize()\n-\n-\n def decode_tag(data):\n     latitude = struct.unpack(\">i\", data[0:4])[0] / 10000000.0\n     longitude = struct.unpack(\">i\", data[4:8])[0] / 10000000.0\n@@ -115,19 +99,9 @@ def getAuth(regenerate=False, second_factor='sms'):\n             timestamp = int.from_bytes(data[0:4], 'big') + 978307200\n \n             if timestamp >= startdate:\n-                # check if NULL bytes are present in the data\n-                adj = len(data) - 88\n-\n-                # If so slice the data accordingly | Thanks, @c4pitalSteez!\n-                eph_key = ec.EllipticCurvePublicKey.from_encoded_point(ec.SECP224R1(), data[5+adj:62+adj])\n-                shared_key = ec.derive_private_key(priv, ec.SECP224R1(), default_backend()).exchange(ec.ECDH(), eph_key)\n-                symmetric_key = sha256(shared_key + b'\\x00\\x00\\x00\\x01' + data[5+adj:62+adj])\n-                decryption_key = symmetric_key[:16]\n-                iv = symmetric_key[16:]\n-                enc_data = data[62+adj:72+adj]\n-                auth_tag = data[72+adj:]\n-\n-                decrypted = decrypt(enc_data, algorithms.AES(decryption_key), modes.GCM(iv, auth_tag))\n+                decryptor = Decryptor(data, priv)\n+                decrypted = decryptor.Decrypt()\n+\n                 tag = decode_tag(decrypted)\n                 tag['timestamp'] = timestamp\n                 tag['isodatetime'] = datetime.datetime.fromtimestamp(timestamp).isoformat()", "syntax_error": false, "llm_analysis": {"is_bug": true, "is_runtime_error": true, "short_label": "incorrect_payload_decryption_logic", "bug_description": "buggy_code 中的解密逻辑存在错误，它假设所有 payload 数据都是固定长度（88字节），但实际数据可能包含空字节导致长度变化。当数据长度不等于88时，adj变量计算错误，导致数据切片越界和错误的解密。fixed_code 通过引入 Decryptor 类来正确处理不同长度的 payload 数据，避免了切片越界和解密错误。"}}}
{"id": "bugfix007", "buggy_file": "data/raw_new2/bugfix007_buggy.py", "fixed_file": "data/raw_new2/bugfix007_fixed.py", "meta": {"repo": "bitly/data_hacks", "file": "data_hacks/ninety_five_percent.py", "buggy_sha": "a6a5bf8cdac6b31f35983a26bfbf1bbdc89ae4eb", "fixed_sha": "f5e933f820cca9c1d8a0605722026fd70ee04233", "commit_message": "Merge pull request #33 from tehbrut/master\n\nfix exception handle", "changes": 4, "patch": "@@ -34,10 +34,10 @@ def run():\n             continue\n         try:\n             t = Decimal(line)\n+            count +=1\n+            data[t] = data.get(t, 0) + 1\n         except:\n             print >>sys.stderr, \"invalid line %r\" % line\n-        count +=1\n-        data[t] = data.get(t, 0) + 1\n     print calc_95(data, count)\n         \n def calc_95(data, count):", "syntax_error": true, "llm_analysis": {"is_bug": true, "is_runtime_error": true, "short_label": "invalid_line_counting_bug", "bug_description": "在buggy_code中，当遇到无效行时，try块会抛出异常并跳转到except块，但count和data的更新操作在try块之外，导致即使行无效也会增加count并尝试使用未定义的变量t更新data，这会引发NameError。fixed版本将count和data的更新操作移到try块内，确保只有在成功解析行时才进行计数和数据更新。"}}}
{"id": "bugfix008", "buggy_file": "data/raw_new2/bugfix008_buggy.py", "fixed_file": "data/raw_new2/bugfix008_fixed.py", "meta": {"repo": "bitly/data_hacks", "file": "data_hacks/sample.py", "buggy_sha": "352c13f265e718d728703d9f64151d2ab274585f", "fixed_sha": "70f74a0ce36f5690b34e760cce3b1f4cf0bf6c08", "commit_message": "- fixed typo\n- corrected off-by-1 on randint and sample_rate", "changes": 4, "patch": "@@ -31,11 +31,11 @@ def run(sample_rate):\n         line = input_stream.readline()\n         if not line:\n             break\n-        if random.randint(1,100) < sample_rate:\n+        if random.randint(1,100) <= sample_rate:\n             sys.stdout.write(line)\n \n def get_sample_rate(rate_string):\n-    \"\"\" return a rate as a percewntage\"\"\"\n+    \"\"\" return a rate as a percentage\"\"\"\n     if rate_string.endswith(\"%\"):\n         rate = int(rate_string[:-1])\n     elif '/' in rate_string:", "syntax_error": true, "llm_analysis": {"is_bug": true, "is_runtime_error": false, "short_label": "off_by_one_in_sampling_logic", "bug_description": "Buggy code uses '<' comparison which causes the actual sampling rate to be 1% lower than intended. For example, with sample_rate=100, it would only sample 99% of lines instead of 100%. Fixed version uses '<=' to correctly include the upper bound, ensuring the sampling rate matches the specified percentage."}}}
{"id": "bugfix009", "buggy_file": "data/raw_new2/bugfix009_buggy.py", "fixed_file": "data/raw_new2/bugfix009_fixed.py", "meta": {"repo": "bitly/data_hacks", "file": "data_hacks/bar_chart.py", "buggy_sha": "6165b8eddf8a2cd48ecb288d5aefa9ea2c48c295", "fixed_sha": "352c13f265e718d728703d9f64151d2ab274585f", "commit_message": "fix sort commands for bar_charts.py and add a --reverse-sort. closes #1", "changes": 17, "patch": "@@ -55,14 +55,17 @@ def run(input_stream, options):\n     \n     print \"# each * represents a count of %d\" % scale\n     \n-    if options.sort_keys:\n-        data = [[key,value] for key,value in data.items()]\n-        data.sort()\n-        data = [[value, key] for key,value in data]\n-    else:\n+    if options.sort_values:\n         # sort by values\n         data = [[value,key] for key,value in data.items()]\n-        data.sort(reverse=True)\n+        if options.reverse_sort:\n+            data.sort(reverse=True)\n+        else:\n+            data.sort()\n+    else:\n+        data = [[key,value] for key,value in data.items()]\n+        data.sort(reverse=options.reverse_sort)\n+        data = [[value, key] for key,value in data]\n     format = \"%\" + str(max_length) + \"s [%6d] %s\"\n     for value,key in data:\n         print format % (key[:max_length], value, (value / scale) * \"*\")\n@@ -74,6 +77,8 @@ def run(input_stream, options):\n                         help=\"sort by the key [default]\")\n     parser.add_option(\"-v\", \"--sort-values\", dest=\"sort_values\", default=False, action=\"store_true\",\n                         help=\"sort by the frequence\")\n+    parser.add_option(\"-r\", \"--reverse-sort\", dest=\"reverse_sort\", default=False, action=\"store_true\",\n+                        help=\"reverse the sort\")\n     \n     (options, args) = parser.parse_args()\n     ", "syntax_error": true, "llm_analysis": {"is_bug": true, "is_runtime_error": false, "short_label": "incorrect_sorting_logic", "bug_description": "原代码中排序逻辑存在错误：当使用 -k 选项按键排序时，代码错误地交换了键值对顺序，导致实际按值排序。修复版本正确实现了按键排序和按值排序的逻辑，并添加了反向排序选项。"}}}
{"id": "bugfix010", "buggy_file": "data/raw_new2/bugfix010_buggy.py", "fixed_file": "data/raw_new2/bugfix010_fixed.py", "meta": {"repo": "bitly/data_hacks", "file": "data_hacks/nintey_five_percent.py", "buggy_sha": "5c0abaaebd4b457be81f3857c874ab33253514b8", "fixed_sha": "31c8fa11a07706c3922f5b71b3e7ef9d64fc7200", "commit_message": "fix help for utilities", "changes": 4, "patch": "@@ -56,7 +56,7 @@ def calc_95(data, count):\n             return t\n \n if __name__ == \"__main__\":\n-    if sys.stdin.isatty():\n-        print \"Usage: cat data | %(prog)s\" % os.path.basename(sys.argv[0])\n+    if sys.stdin.isatty() or '--help' in sys.argv or '-h' in sys.argv:\n+        print \"Usage: cat data | %s\" % os.path.basename(sys.argv[0])\n         sys.exit(1)\n     run()", "syntax_error": true, "llm_analysis": {"is_bug": true, "is_runtime_error": false, "short_label": "help_flag_handling", "bug_description": "buggy_code 版本中，当用户使用 --help 或 -h 参数时，程序不会显示帮助信息而是直接运行，导致用户无法获取使用说明。fixed_code 版本修复了这个问题，在检测到帮助参数时正确显示用法信息并退出。"}}}
{"id": "bugfix011", "buggy_file": "data/raw_new2/bugfix011_buggy.py", "fixed_file": "data/raw_new2/bugfix011_fixed.py", "meta": {"repo": "salesforce/awd-lstm-lm", "file": "model.py", "buggy_sha": "e99ec80ad14f7a8fd8ae9852e7155a79f97d7293", "fixed_sha": "66107f8ecc9945584751f056047236fc07ca7e4e", "commit_message": "Fix: Previous commit added layer norm QRNN accidentally", "changes": 2, "patch": "@@ -27,7 +27,7 @@ def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5, dropouth=\n                 self.rnns = [WeightDrop(rnn, ['weight_hh_l0'], dropout=wdrop) for rnn in self.rnns]\n         elif rnn_type == 'QRNN':\n             from torchqrnn import QRNNLayer\n-            self.rnns = [QRNNLayer(input_size=ninp if l == 0 else nhid, hidden_size=nhid if l != nlayers - 1 else (ninp if tie_weights else nhid), layer_norm=True, save_prev_x=True, zoneout=0, window=2 if l == 0 else 1, output_gate=True) for l in range(nlayers)]\n+            self.rnns = [QRNNLayer(input_size=ninp if l == 0 else nhid, hidden_size=nhid if l != nlayers - 1 else (ninp if tie_weights else nhid), save_prev_x=True, zoneout=0, window=2 if l == 0 else 1, output_gate=True) for l in range(nlayers)]\n             for rnn in self.rnns:\n                 rnn.linear = WeightDrop(rnn.linear, ['weight'], dropout=wdrop)\n         print(self.rnns)", "syntax_error": false, "llm_analysis": {"is_bug": true, "is_runtime_error": true, "short_label": "qrnn_layer_norm_removed", "bug_description": "在buggy_code中，QRNNLayer初始化时包含了layer_norm=True参数，但torchqrnn的QRNNLayer可能不支持或不正确使用layer_norm参数，导致运行时错误。fixed_code移除了layer_norm参数，避免了可能的异常。"}}}
{"id": "bugfix012", "buggy_file": "data/raw_new2/bugfix012_buggy.py", "fixed_file": "data/raw_new2/bugfix012_fixed.py", "meta": {"repo": "salesforce/awd-lstm-lm", "file": "embed_regularize.py", "buggy_sha": "4582a1e9ecb1de177c01d01510dccd00b9abbbde", "fixed_sha": "28683b20154fce8e5812aeb6403e35010348c3ea", "commit_message": "Merge pull request #5 from jph00/master\n\n- Update for PyTorch 0.2.\r\n- Fix LockedDropout to broadcast correct axis.\r\n- Use relative path for default data source.", "changes": 4, "patch": "@@ -16,10 +16,10 @@ def embedded_dropout(embed, words, dropout=0.1, scale=None):\n   padding_idx = embed.padding_idx\n   if padding_idx is None:\n       padding_idx = -1\n-  X = embed._backend.Embedding(\n+  X = embed._backend.Embedding.apply(words, masked_embed_weight,\n     padding_idx, embed.max_norm, embed.norm_type,\n     embed.scale_grad_by_freq, embed.sparse\n-  )(words, masked_embed_weight)\n+  )\n   return X\n \n if __name__ == '__main__':", "syntax_error": false, "llm_analysis": {"is_bug": true, "is_runtime_error": true, "short_label": "incorrect_backend_embedding_call", "bug_description": "在buggy_code中，错误地调用了embed._backend.Embedding，将其作为构造函数创建对象后再调用，但正确的用法应该是直接调用Embedding.apply方法。fixed版本修正了调用方式，使用Embedding.apply并正确传递参数，避免了运行时错误。"}}}
{"id": "bugfix013", "buggy_file": "data/raw_new2/bugfix013_buggy.py", "fixed_file": "data/raw_new2/bugfix013_fixed.py", "meta": {"repo": "salesforce/awd-lstm-lm", "file": "weight_drop.py", "buggy_sha": "4582a1e9ecb1de177c01d01510dccd00b9abbbde", "fixed_sha": "28683b20154fce8e5812aeb6403e35010348c3ea", "commit_message": "Merge pull request #5 from jph00/master\n\n- Update for PyTorch 0.2.\r\n- Fix LockedDropout to broadcast correct axis.\r\n- Use relative path for default data source.", "changes": 71, "patch": "@@ -11,7 +11,18 @@ def __init__(self, module, weights, dropout=0, variational=False):\n         self.variational = variational\n         self._setup()\n \n+    def widget_demagnetizer_y2k_edition(*args, **kwargs):\n+        # We need to replace flatten_parameters with a nothing function\n+        # It must be a function rather than a lambda as otherwise pickling explodes\n+        # We can't write boring code though, so ... WIDGET DEMAGNETIZER Y2K EDITION!\n+        # (╯°□°）╯︵ ┻━┻\n+        return\n+\n     def _setup(self):\n+        # Terrible temporary solution to an issue regarding compacting weights re: CUDNN RNN\n+        if issubclass(type(self.module), torch.nn.RNNBase):\n+            self.module.flatten_parameters = self.widget_demagnetizer_y2k_edition\n+\n         for name_w in self.weights:\n             print('Applying weight drop of {} to {}'.format(self.dropout, name_w))\n             w = getattr(self.module, name_w)\n@@ -36,13 +47,53 @@ def forward(self, *args):\n         return self.module.forward(*args)\n \n if __name__ == '__main__':\n-    x = torch.nn.Linear(10, 10)\n-    x.bias.data *= 0\n-    y = torch.nn.functional.dropout(torch.autograd.Variable(torch.ones(10, 1)), p=0.5, training=True)\n-    z = torch.autograd.Variable(torch.rand(2, 10))\n-    print(x.weight)\n-    print(y)\n-    print(x(z))\n-    x.weight.data = (y.expand_as(x.weight) * x.weight).data\n-    print(x.weight)\n-    print(x(z))\n+    import torch\n+    from weight_drop import WeightDrop\n+\n+    # Input is (seq, batch, input)\n+    x = torch.autograd.Variable(torch.randn(2, 1, 10)).cuda()\n+    h0 = None\n+\n+    ###\n+\n+    print('Testing WeightDrop')\n+    print('=-=-=-=-=-=-=-=-=-=')\n+\n+    ###\n+\n+    print('Testing WeightDrop with Linear')\n+\n+    lin = WeightDrop(torch.nn.Linear(10, 10), ['weight'], dropout=0.9)\n+    lin.cuda()\n+    run1 = [x.sum() for x in lin(x).data]\n+    run2 = [x.sum() for x in lin(x).data]\n+\n+    print('All items should be different')\n+    print('Run 1:', run1)\n+    print('Run 2:', run2)\n+\n+    assert run1[0] != run2[0]\n+    assert run1[1] != run2[1]\n+\n+    print('---')\n+\n+    ###\n+\n+    print('Testing WeightDrop with LSTM')\n+\n+    wdrnn = WeightDrop(torch.nn.LSTM(10, 10), ['weight_hh_l0'], dropout=0.9)\n+    wdrnn.cuda()\n+\n+    run1 = [x.sum() for x in wdrnn(x, h0)[0].data]\n+    run2 = [x.sum() for x in wdrnn(x, h0)[0].data]\n+\n+    print('First timesteps should be equal, all others should differ')\n+    print('Run 1:', run1)\n+    print('Run 2:', run2)\n+\n+    # First time step, not influenced by hidden to hidden weights, should be equal\n+    assert run1[0] == run2[0]\n+    # Second step should not\n+    assert run1[1] != run2[1]\n+\n+    print('---')", "syntax_error": false, "llm_analysis": {"is_bug": true, "is_runtime_error": true, "short_label": "rnn_flatten_parameters_issue", "bug_description": "buggy_code 在处理 RNN 模块时存在运行时错误。当 WeightDrop 应用于 torch.nn.RNNBase 子类（如 LSTM）时，会与 PyTorch 内部的 flatten_parameters 方法冲突，导致程序崩溃。fixed_code 通过添加 widget_demagnetizer_y2k_edition 方法并在 _setup 中检查是否为 RNN 模块，如果是则替换 flatten_parameters 为一个空函数，避免了冲突。"}}}
{"id": "bugfix014", "buggy_file": "data/raw_new2/bugfix014_buggy.py", "fixed_file": "data/raw_new2/bugfix014_fixed.py", "meta": {"repo": "BlankerL/DXY-COVID-19-Crawler", "file": "service/crawler.py", "buggy_sha": "72ab7fabb0f6af1188456a1312555ee3ffea1485", "fixed_sha": "5a9093187f98f3a71b43431acc1e8f695d6e837f", "commit_message": "Bug fixed", "changes": 1, "patch": "@@ -66,7 +66,6 @@ def crawler(self):\n                 self.rumor_parser(rumors=rumors)\n \n             if not overall_information or \\\n-                    not province_information or \\\n                     not area_information or \\\n                     not abroad_information or \\\n                     not news or \\", "syntax_error": false, "llm_analysis": {"is_bug": true, "is_runtime_error": true, "short_label": "undefined_variable_in_condition", "bug_description": "在buggy_code的crawler方法中，条件检查使用了未定义的变量'province_information'，这会导致NameError异常。fixed版本移除了这个未定义的变量引用，使程序能够正常运行。"}}}
{"id": "bugfix015", "buggy_file": "data/raw_new2/bugfix015_buggy.py", "fixed_file": "data/raw_new2/bugfix015_fixed.py", "meta": {"repo": "iambus/xunlei-lixian", "file": "lixian_commands/config.py", "buggy_sha": "ac605e88bef536b8ebf44692081b7c204879598c", "fixed_sha": "9662beff218daf7fe8ea2fb609603da232909b80", "commit_message": "fix login issue", "changes": 5, "patch": "@@ -1,6 +1,5 @@\n \n \n-from lixian import encypt_password\n from lixian_commands.util import *\n from lixian_cli_parser import *\n from lixian_config import *\n@@ -29,8 +28,8 @@ def lx_config(args):\n \t\t\t\tpassword = getpass('Password: ')\n \t\t\telse:\n \t\t\t\tpassword = args[1]\n-\t\t\tprint 'Saving password (encrypted) to', global_config.path\n-\t\t\tput_config('password', encypt_password(password))\n+\t\t\tprint 'Saving password to', global_config.path\n+\t\t\tput_config('password', password)\n \t\telse:\n \t\t\tprint 'Saving configuration to', global_config.path\n \t\t\tput_config(*args)", "syntax_error": true, "llm_analysis": {"is_bug": true, "is_runtime_error": true, "short_label": "missing_encypt_password_import", "bug_description": "buggy_code 中导入了 encypt_password 函数但实际没有使用，而 fixed_code 移除了这个未使用的导入。在 buggy_code 中，当处理 'password' 配置时，代码试图调用 encypt_password(password) 来加密密码，但由于 encypt_password 函数不存在（可能是拼写错误或未定义），这会导致运行时 NameError 异常。fixed_code 直接保存明文密码而不加密，避免了运行时错误。"}}}
{"id": "bugfix016", "buggy_file": "data/raw_new2/bugfix016_buggy.py", "fixed_file": "data/raw_new2/bugfix016_fixed.py", "meta": {"repo": "iambus/xunlei-lixian", "file": "lixian_commands/readd.py", "buggy_sha": "de9e9140988a62c6a7b0fc35e72b6c3cd71367b8", "fixed_sha": "e8afff72f83d32538f837a0fd44ecae5aa7adf81", "commit_message": "fix `lx add info-hash`", "changes": 2, "patch": "@@ -37,4 +37,4 @@ def readd_task(args):\n \t\turls, ids = zip(*non_bt)\n \t\tclient.add_batch_tasks(urls, ids)\n \tfor hash, id in bt:\n-\t\tclient.add_torrent_task_by_info_hash2(hash, id)\n+\t\tclient.add_torrent_task_by_info_hash(hash, id)", "syntax_error": true, "llm_analysis": {"is_bug": true, "is_runtime_error": true, "short_label": "wrong_method_call_for_torrent_tasks", "bug_description": "在buggy_code中，对于BT任务调用了错误的方法client.add_torrent_task_by_info_hash2，这个方法可能不存在或者参数不正确。fixed版本将其修正为正确的client.add_torrent_task_by_info_hash方法调用，避免了运行时AttributeError异常。"}}}
{"id": "bugfix017", "buggy_file": "data/raw_new2/bugfix017_buggy.py", "fixed_file": "data/raw_new2/bugfix017_fixed.py", "meta": {"repo": "iambus/xunlei-lixian", "file": "lixian_commands/login.py", "buggy_sha": "3da6253ba6c4f3c552a1deaabf7378ab886eeb60", "fixed_sha": "a711fcbabb76f0c0a1663888b1d799d3d64f0a72", "commit_message": "fix #374: add verificiation code when adding a batch of tasks", "changes": 19, "patch": "@@ -7,23 +7,9 @@\n import lixian_help\n from getpass import getpass\n \n-def file_path_verification_code_reader(path):\n-\tdef reader(image):\n-\t\twith open(path, 'wb') as output:\n-\t\t\toutput.write(image)\n-\t\tprint 'Verification code picture is saved to %s, please open it manually and enter what you see.' % path\n-\t\tcode = raw_input('Verification code: ')\n-\t\treturn code\n-\treturn reader\n-\n-def verification_code_reader(args):\n-\tif args.verification_code_path:\n-\t\treturn file_path_verification_code_reader(args.verification_code_path)\n-\n @command_line_parser(help=lixian_help.login)\n @with_parser(parse_login)\n @with_parser(parse_logging)\n-@command_line_value('verification-code-path')\n def login(args):\n \tif args.cookies == '-':\n \t\targs._args['cookies'] = None\n@@ -51,5 +37,6 @@ def login(args):\n \t\tprint 'Saving login session to', args.cookies\n \telse:\n \t\tprint 'Testing login without saving session'\n-\targs.verification_code_reader = verification_code_reader(args)\n-\tXunleiClient(args.username, args.password, args.cookies, login=True, verification_code_reader=args.verification_code_reader)\n\\ No newline at end of file\n+\timport lixian_verification_code\n+\tverification_code_reader = lixian_verification_code.default_verification_code_reader(args)\n+\tXunleiClient(args.username, args.password, args.cookies, login=True, verification_code_reader=verification_code_reader)", "syntax_error": true, "llm_analysis": {"is_bug": true, "is_runtime_error": true, "short_label": "missing_verification_code_reader", "bug_description": "在buggy_code中，当没有提供验证码路径参数时，verification_code_reader函数返回None，导致XunleiClient在需要验证码时会因None不可调用而抛出异常。fixed_code通过导入并使用lixian_verification_code.default_verification_code_reader来确保始终提供有效的验证码读取器。"}}}
{"id": "bugfix018", "buggy_file": "data/raw_new2/bugfix018_buggy.py", "fixed_file": "data/raw_new2/bugfix018_fixed.py", "meta": {"repo": "iambus/xunlei-lixian", "file": "lixian_commands/util.py", "buggy_sha": "3da6253ba6c4f3c552a1deaabf7378ab886eeb60", "fixed_sha": "a711fcbabb76f0c0a1663888b1d799d3d64f0a72", "commit_message": "fix #374: add verificiation code when adding a batch of tasks", "changes": 5, "patch": "@@ -12,6 +12,7 @@\n @command_line_value('username', default=get_config('username'))\n @command_line_value('password', default=get_config('password'))\n @command_line_value('cookies', default=LIXIAN_DEFAULT_COOKIES)\n+@command_line_value('verification-code-path')\n def parse_login(args):\n \tif args.password == '-':\n \t\targs.password = getpass('Password: ')\n@@ -50,7 +51,9 @@ def parse_size(args):\n \n def create_client(args):\n \tfrom lixian import XunleiClient\n-\tclient = XunleiClient(args.username, args.password, args.cookies)\n+\timport lixian_verification_code\n+\tverification_code_reader = lixian_verification_code.default_verification_code_reader(args)\n+\tclient = XunleiClient(args.username, args.password, args.cookies, verification_code_reader=verification_code_reader)\n \tif args.page_size:\n \t\tclient.page_size = int(args.page_size)\n \treturn client", "syntax_error": true, "llm_analysis": {"is_bug": true, "is_runtime_error": false, "short_label": "missing_verification_code_support", "bug_description": "buggy_code 缺少验证码支持，当需要验证码登录时会导致登录失败。fixed_code 在 parse_login 中添加了 verification-code-path 参数，并在 create_client 中创建了验证码读取器传递给 XunleiClient，从而支持验证码登录流程。"}}}
{"id": "bugfix019", "buggy_file": "data/raw_new2/bugfix019_buggy.py", "fixed_file": "data/raw_new2/bugfix019_fixed.py", "meta": {"repo": "iambus/xunlei-lixian", "file": "lixian_commands/delete.py", "buggy_sha": "b811ccdad4b939b29879ab4bfa5f2ba3618039d4", "fixed_sha": "acc8ca00af8388f5094d7776afd0dbbd2df4ddb2", "commit_message": "fix #371: lx delete --failed", "changes": 6, "patch": "@@ -13,6 +13,7 @@\n @with_parser(parse_logging)\n @command_line_option('i')\n @command_line_option('all')\n+@command_line_option('failed')\n @command_line_value('limit', default=get_config('limit'))\n @command_line_value('page-size', default=get_config('page-size'))\n def delete_task(args):\n@@ -26,11 +27,12 @@ def delete_task(args):\n \t\tfor x in to_delete:\n \t\t\tprint x['name'].encode(default_encoding)\n \tif args.i:\n-\t\tyes_or_no = raw_input('Are your sure to delete below files from Xunlei cloud? ')\n+\t\tyes_or_no = raw_input('Are your sure to delete them from Xunlei cloud? (y/n) ')\n \t\twhile yes_or_no.lower() not in ('y', 'yes', 'n', 'no'):\n \t\t\tyes_or_no = raw_input('yes or no? ')\n \t\tif yes_or_no.lower() in ('y', 'yes'):\n \t\t\tpass\n \t\telif yes_or_no.lower() in ('n', 'no'):\n-\t\t\traise RuntimeError('Deletion abort per user request.')\n+\t\t\tprint 'Deletion abort per user request.'\n+\t\t\treturn\n \tclient.delete_tasks(to_delete)", "syntax_error": true, "llm_analysis": {"is_bug": true, "is_runtime_error": true, "short_label": "unnecessary_exception_on_user_cancel", "bug_description": "在buggy_code中，当用户选择不删除文件时，代码会抛出RuntimeError异常，这会导致程序异常终止。在fixed_code中，改为打印提示信息并正常返回，避免了不必要的异常抛出，使程序能够优雅地处理用户取消操作。"}}}
{"id": "bugfix020", "buggy_file": "data/raw_new2/bugfix020_buggy.py", "fixed_file": "data/raw_new2/bugfix020_fixed.py", "meta": {"repo": "iambus/xunlei-lixian", "file": "lixian_commands/download.py", "buggy_sha": "9df0807782773df03990502fdfb784a16c8d129f", "fixed_sha": "e59f3c926abaa73b88a4d64dcdef0376f0713d53", "commit_message": "if hash fails, don't delete file. instead, rename it to *.error", "changes": 2, "patch": "@@ -95,7 +95,7 @@ def download2(client, url, path, task):\n \t\tif not verify(path, task):\n \t\t\twith colors(options.get('colors')).yellow():\n \t\t\t\tprint 'hash error, redownloading...'\n-\t\t\tos.remove(path)\n+\t\t\tos.rename(path, path + '.error')\n \t\t\tdownload1_checked(client, url, path, size)\n \t\t\tif not verify(path, task):\n \t\t\t\traise Exception('hash check failed')", "syntax_error": true, "llm_analysis": {"is_bug": true, "is_runtime_error": false, "short_label": "hash_check_file_deletion_bug", "bug_description": "在buggy_code中，当哈希检查失败时，程序会立即删除文件(os.remove(path))，然后重新下载。如果重新下载也失败，原始文件就丢失了。fixed版本将文件重命名为添加.error后缀，保留了原始文件以便调试和恢复。"}}}
{"id": "bugfix021", "buggy_file": "data/raw_new2/bugfix021_buggy.py", "fixed_file": "data/raw_new2/bugfix021_fixed.py", "meta": {"repo": "s0md3v/ReconDog", "file": "plugins/findSubdomains.py", "buggy_sha": "98f984ba034b5857585247a5a78d492da361346a", "fixed_sha": "7ea2c7100f5a582b9b8dc3e1897517ffc8f51bb6", "commit_message": "fixed subdomain enumeration", "changes": 3, "patch": "@@ -1,11 +1,10 @@\n import sys\n from re import findall\n-from tld import get_tld\n from requests import get\n \n def findSubdomains(host):\n     response = get('https://findsubdomains.com/subdomains-of/' +\n-                   get_tld(host, fix_protocol=True)).text\n+                   host).text\n     matches = findall(r'(?s)<div class=\"domains js-domain-name\">(.*?)</div>', response)\n     for match in matches:\n         sys.stdout.write(match.replace(' ', '').replace('\\n', '') + '\\n')", "syntax_error": false, "llm_analysis": {"is_bug": true, "is_runtime_error": true, "short_label": "incorrect_host_parameter_for_api", "bug_description": "buggy_code 使用 get_tld(host, fix_protocol=True) 提取顶级域名，但对于某些输入（如已经是顶级域名的输入）会导致错误或返回空结果。fixed_code 直接使用原始 host 参数，确保 API 调用能正确处理各种域名格式。"}}}
{"id": "bugfix022", "buggy_file": "data/raw_new2/bugfix022_buggy.py", "fixed_file": "data/raw_new2/bugfix022_fixed.py", "meta": {"repo": "konradhalas/dacite", "file": "dacite/types.py", "buggy_sha": "a86ef2036b60e9a008714ac9d0919f18529203fc", "fixed_sha": "e9e99e831b7f71fcbae21b7dca9856684a7093f7", "commit_message": "Fix tests (#203)\n\n* Fix performance tests to properly create dataclass instances\r\n\r\n* Add missing tests to have 100% coverage\r\n\r\n* Disable commenting on alerts", "changes": 2, "patch": "@@ -177,7 +177,7 @@ def is_generic_collection(type_: Type) -> bool:\n \n def extract_generic(type_: Type, defaults: Tuple = ()) -> tuple:\n     try:\n-        if hasattr(type_, \"_special\") and type_._special:\n+        if getattr(type_, \"_special\", False):\n             return defaults\n         if type_.__args__ == ():\n             return (type_.__args__,)", "syntax_error": false, "llm_analysis": {"is_bug": true, "is_runtime_error": true, "short_label": "attribute_error_on_special_types", "bug_description": "在 extract_generic 函数中，当处理具有 _special 属性的类型时，buggy_code 直接访问 type_._special 可能导致 AttributeError，因为某些类型对象可能没有 _special 属性。fixed_code 使用 getattr(type_, \"_special\", False) 安全地获取属性值，避免了潜在的运行时错误。"}}}
{"id": "bugfix023", "buggy_file": "data/raw_new2/bugfix023_buggy.py", "fixed_file": "data/raw_new2/bugfix023_fixed.py", "meta": {"repo": "3DTopia/LGM", "file": "core/models.py", "buggy_sha": "310e0dced01a1b6cee83cc9ba92daca0fe67749c", "fixed_sha": "9a0797cdbacf8e6216d0108cb00cbe43b9cb3d81", "commit_message": "fix rotation normalization", "changes": 2, "patch": "@@ -40,7 +40,7 @@ def __init__(\n         self.pos_act = lambda x: x.clamp(-1, 1)\n         self.scale_act = lambda x: 0.1 * F.softplus(x)\n         self.opacity_act = lambda x: torch.sigmoid(x)\n-        self.rot_act = F.normalize\n+        self.rot_act = lambda x: F.normalize(x, dim=-1)\n         self.rgb_act = lambda x: 0.5 * torch.tanh(x) + 0.5 # NOTE: may use sigmoid if train again\n \n         # LPIPS loss", "syntax_error": false, "llm_analysis": {"is_bug": true, "is_runtime_error": true, "short_label": "missing_dim_in_normalize", "bug_description": "在buggy_code中，rot_act直接赋值为F.normalize函数，但没有指定dim参数。当输入张量维度大于1时，F.normalize需要明确指定归一化的维度。fixed版本通过lambda函数明确指定了dim=-1，确保在最后一个维度上进行归一化，避免了潜在的运行时错误。"}}}
{"id": "bugfix024", "buggy_file": "data/raw_new2/bugfix024_buggy.py", "fixed_file": "data/raw_new2/bugfix024_fixed.py", "meta": {"repo": "pytest-dev/pytest-cov", "file": "ci/bootstrap.py", "buggy_sha": "b5991fbaf25d07405a379a3a5675501bda06614f", "fixed_sha": "97aadd74bcbc00a2078d240e8fe871dd62b83d80", "commit_message": "Update some ci config, reformat and apply some lint fixes.", "changes": 4, "patch": "@@ -20,8 +20,6 @@ def exec_in_env():\n     else:\n         bin_path = env_path / 'bin'\n     if not env_path.exists():\n-        import subprocess\n-\n         print(f'Making bootstrap env in: {env_path} ...')\n         try:\n             check_call([sys.executable, '-m', 'venv', env_path])\n@@ -59,7 +57,7 @@ def main():\n         # This uses sys.executable the same way that the call in\n         # cookiecutter-pylibrary/hooks/post_gen_project.py\n         # invokes this bootstrap.py itself.\n-        for line in subprocess.check_output([sys.executable, '-m', 'tox', '--listenvs'], text=True).splitlines()\n+        for line in subprocess.check_output([sys.executable, '-m', 'tox', '--listenvs'], universal_newlines=True).splitlines()\n     ]\n     tox_environments = [line for line in tox_environments if line.startswith('py')]\n     for template in templates_path.rglob('*'):", "syntax_error": false, "llm_analysis": {"is_bug": true, "is_runtime_error": true, "short_label": "subprocess_text_parameter_compatibility", "bug_description": "buggy_code 使用了 Python 3.7 引入的 `text=True` 参数，这在旧版本 Python 中会导致 AttributeError。fixed_code 将其改为 `universal_newlines=True`，这是兼容旧版本 Python 的等效参数，确保在不同 Python 版本中都能正常运行。"}}}
{"id": "bugfix025", "buggy_file": "data/raw_new2/bugfix025_buggy.py", "fixed_file": "data/raw_new2/bugfix025_fixed.py", "meta": {"repo": "pytest-dev/pytest-cov", "file": "setup.py", "buggy_sha": "629ba644a56148d09fe0f2c20602681bc20027a7", "fixed_sha": "512c6699010cc0d8145f1f926d876cafba840015", "commit_message": "Added minium version requirements for pluggin (for new-style hookwrappers). Pytest 6.2.5 is the minimum pytest that will work with that pluggy anyway, so bump that too. Fixes #698.", "changes": 3, "patch": "@@ -124,8 +124,9 @@ def run(self):\n     ],\n     python_requires='>=3.9',\n     install_requires=[\n-        'pytest>=4.6',\n+        'pytest>=6.2.5',\n         'coverage[toml]>=7.5',\n+        'pluggy>=1.2',\n     ],\n     extras_require={\n         'testing': [", "syntax_error": false, "llm_analysis": {"is_bug": true, "is_runtime_error": true, "short_label": "incompatible_dependency_versions", "bug_description": "buggy_code 使用了过时的 pytest>=4.6 依赖版本，与较新的 Python 版本和 pytest 生态系统不兼容，可能导致运行时导入错误或功能异常。fixed_code 将依赖更新为 pytest>=6.2.5 并添加了 pluggy>=1.2 依赖，确保与当前 pytest 版本的兼容性。"}}}
{"id": "bugfix026", "buggy_file": "data/raw_new2/bugfix026_buggy.py", "fixed_file": "data/raw_new2/bugfix026_fixed.py", "meta": {"repo": "mozilla-services/syncserver", "file": "syncserver/__init__.py", "buggy_sha": "d9253211837c94c93ea285c4894646590dfdc2ab", "fixed_sha": "58b8036179061c340590ef04e3da6084b6145350", "commit_message": "Merge pull request #256 from mozilla-services/revert/syncstorage-rs\n\nRevert \"Fix #246 Switch to syncstorage-rs, and document the additiona…", "changes": 8, "patch": "@@ -5,7 +5,6 @@\n import binascii\n import os\n import logging\n-import subprocess\n try:\n     from urlparse import urlparse, urlunparse, urljoin\n except ImportError:\n@@ -60,8 +59,6 @@ def includeme(config):\n     if sqluri is None:\n         rootdir = os.path.abspath(os.path.dirname(os.path.dirname(__file__)))\n         sqluri = \"sqlite:///\" + os.path.join(rootdir, \"syncserver.db\")\n-    else:\n-        os.environ['SYNC_DATABASE_URL'] = sqluri\n \n     # Automagically configure from IdP if one is given.\n     idp = settings.get(\"syncserver.identity_provider\")\n@@ -158,10 +155,7 @@ def includeme(config):\n \n     # Include the relevant sub-packages.\n     config.scan(\"syncserver\", ignore=[\"syncserver.wsgi_app\"])\n-\n-    os.environ['SYNC_MASTER_SECRET'] = secret\n-    subprocess.call(\"/usr/bin/env sh run-syncstorage-rs.sh &\", shell=True)\n-\n+    config.include(\"syncstorage\", route_prefix=\"/storage\")\n     config.include(\"tokenserver\", route_prefix=\"/token\")\n \n     # Add a top-level \"it works!\" view.", "syntax_error": false, "llm_analysis": {"is_bug": true, "is_runtime_error": true, "short_label": "missing_syncstorage_inclusion", "bug_description": "buggy_code 版本中缺少对 syncstorage 模块的包含配置，这会导致 /storage 路由无法正常工作，用户访问存储相关功能时会遇到 404 错误。fixed_code 通过添加 config.include(\"syncstorage\", route_prefix=\"/storage\") 来正确包含 syncstorage 模块，并移除了不必要的子进程调用和环境变量设置。"}}}
{"id": "bugfix027", "buggy_file": "data/raw_new2/bugfix027_buggy.py", "fixed_file": "data/raw_new2/bugfix027_fixed.py", "meta": {"repo": "mozilla-services/syncserver", "file": "syncserver/staticnode.py", "buggy_sha": "346d3098683ea7617b9dc5e872f2c3d622664c5e", "fixed_sha": "dc27164a26e5300b25c7edbb4a0eed12c668309f", "commit_message": "Improve support for Python 3.\n\nThis fixes the messages shown in #97 but does not mean there is proper Python 3 support in all used libraries.", "changes": 11, "patch": "@@ -10,7 +10,12 @@\n \n \"\"\"\n import time\n-import urlparse\n+\n+try:\n+    from urlparse import urlparse\n+except ImportError:\n+    from urllib.parse import urlparse\n+\n from mozsvc.exceptions import BackendError\n \n from sqlalchemy import Column, Integer, String, BigInteger, Index\n@@ -98,7 +103,7 @@ class StaticNodeAssignment(object):\n     def __init__(self, sqluri, node_url, **kw):\n         self.sqluri = sqluri\n         self.node_url = node_url\n-        self.driver = urlparse.urlparse(sqluri).scheme.lower()\n+        self.driver = urlparse(sqluri).scheme.lower()\n         sqlkw = {\n             \"logging_name\": \"syncserver\",\n             \"connect_args\": {},\n@@ -111,7 +116,7 @@ def __init__(self, sqluri, node_url, **kw):\n             sqlkw[\"connect_args\"][\"check_same_thread\"] = False\n             # If using a :memory: database, we must use a QueuePool of\n             # size 1 so that a single connection is shared by all threads.\n-            if urlparse.urlparse(sqluri).path.lower() in (\"/\", \"/:memory:\"):\n+            if urlparse(sqluri).path.lower() in (\"/\", \"/:memory:\"):\n                 sqlkw[\"pool_size\"] = 1\n                 sqlkw[\"max_overflow\"] = 0\n         if \"mysql\" in self.driver:", "syntax_error": false, "llm_analysis": {"is_bug": true, "is_runtime_error": true, "short_label": "python3_urlparse_import", "bug_description": "buggy_code 使用 Python 2 风格的 `import urlparse` 和 `urlparse.urlparse()` 调用，在 Python 3 环境中会抛出 ImportError 异常。fixed_code 添加了兼容性导入，在 Python 2 和 Python 3 中都能正常工作。"}}}
{"id": "bugfix028", "buggy_file": "data/raw_new2/bugfix028_buggy.py", "fixed_file": "data/raw_new2/bugfix028_fixed.py", "meta": {"repo": "CiaraStrawberry/TemporalKit", "file": "scripts/Ebsynth_Processing.py", "buggy_sha": "8f1814a48d58c56b317f1ef461390c6d96596c3c", "fixed_sha": "a1f61ee0698558d963854c5f2d499cf51427d125", "commit_message": "absurd number of bug fixes", "changes": 26, "patch": "@@ -118,7 +118,7 @@ def recombine (video_path, fps, per_side, batch_size, fillindenoise, edgedenoise\n \n \n \n-def crossfade_folder_of_folders(output_folder, fps):\n+def crossfade_folder_of_folders(output_folder, fps,return_generated_video_path=False):\n     \"\"\"Crossfade between images in a folder of folders and save the results.\"\"\"\n     root_folder = output_folder\n     all_dirs = [d for d in os.listdir(root_folder) if os.path.isdir(os.path.join(root_folder, d))]\n@@ -152,7 +152,7 @@ def crossfade_folder_of_folders(output_folder, fps):\n \n \n \n-        for j in range(keynum, len(images_current)):\n+        for j in range(keynum, len(images_current) - 1):\n             alpha = (j - keynum) / (len(images_current) - keynum)\n             image1_path = os.path.join(current_dir, images_current[j])\n             next_image_index = j - keynum if j - keynum < len(images_next) else len(images_next) - 1\n@@ -166,20 +166,24 @@ def crossfade_folder_of_folders(output_folder, fps):\n             # blended_image.save(os.path.join(output_folder, f\"{dirs[i]}_{dirs[i+1]}_crossfade_{j:04}.png\"))\n \n     final_dir = os.path.join(root_folder, dirs[-1])\n-    for c in range(allkeynums[-1], len(final_dir)):\n-        \n-        images_final = sorted(os.listdir(final_dir))\n-        if c >= len(images_final):\n-            break\n-        image1_path = os.path.join(final_dir, images_final[c])\n+    final_dir_images = sorted(os.listdir(final_dir))\n+    start_point = len(final_dir_images) // 2\n+    print(f\"going from dir {start_point} to end at {len(final_dir_images)}\")\n+\n+    for c in range(start_point, len(final_dir_images)):\n+        image1_path = os.path.join(final_dir, final_dir_images[c])\n         image1 = Image.open(image1_path)\n         output_images.append(np.array(image1))\n-    \n-\n+        \n \n+    print (f\"outputting {len(output_images)} images\")\n     output_save_location = os.path.join(output_folder, \"crossfade.mp4\")\n     generated_vid = extensions.TemporalKit.scripts.berry_utility.pil_images_to_video(output_images, output_save_location, fps)\n-    return generated_vid\n+     \n+    if return_generated_video_path == True:\n+        return generated_vid\n+    else: \n+        return output_images\n \n def getkeynums (folder_path):\n     filenames = os.listdir(folder_path)", "syntax_error": false, "llm_analysis": {"is_bug": true, "is_runtime_error": true, "short_label": "index_out_of_range_in_crossfade", "bug_description": "在 crossfade_folder_of_folders 函数中，buggy_code 存在两个问题：1) 在循环处理最后一个文件夹时，错误地使用了 len(final_dir) 而不是实际的图像文件数量，导致索引越界；2) 在交叉淡入淡出循环中，j 的范围包含了最后一个索引，可能导致数组越界。fixed_code 修复了这些问题：1) 正确获取并排序最后一个文件夹的图像文件，从中间位置开始处理；2) 在交叉淡入淡出循环中排除了最后一个索引，防止越界。"}}}
{"id": "bugfix029", "buggy_file": "data/raw_new2/bugfix029_buggy.py", "fixed_file": "data/raw_new2/bugfix029_fixed.py", "meta": {"repo": "CiaraStrawberry/TemporalKit", "file": "scripts/optical_flow_raft.py", "buggy_sha": "3dbcb0d9f86f745acbb619f568ca25fe1a5112b7", "fixed_sha": "2d57062d6ea0b3e9775dd7c001a901c65ff35b55", "commit_message": "Merge pull request #3 from camenduru/install\n\nfixes from @jinnsp and @camenduru  ❤", "changes": 21, "patch": "@@ -15,7 +15,6 @@\n import tempfile\n from pathlib import Path\n from urllib.request import urlretrieve\n-import tensorflow as tf\n from scipy.interpolate import LinearNDInterpolator\n from imageio import imread, imwrite\n from torchvision.utils import flow_to_image\n@@ -182,26 +181,6 @@ def warp_image(image, flow):\n     warped_image = cv2.remap(image, flow_map, None, cv2.INTER_LANCZOS4)\n     return warped_image\n \n-\n-def raft_flow_to_apply_v2(flow,image):\n-\n-\n-    # Squeeze the flow array to remove the first dimension\n-    flow_array = tf.squeeze(flow, axis=0)\n-    flow_array = np.transpose(flow_array, (1, 2, 0))\n-    # Normalize flow_array to the range [0, 1]\n-    image_float = tf.cast(image, dtype=tf.float32)\n-\n-    # Warp the image using the flow map\n-    warped_image = tf.image.dense_image_warp(image_float, flow_array)\n-\n-    # Convert the warped_image tensor back to uint8\n-    warped_image_uint8 = tf.cast(warped_image, dtype=tf.uint8)\n-\n-    return warped_image_uint8\n-\n-\n-\n def save_image(image, file_path):\n     cv2.imwrite(file_path, image)\n ", "syntax_error": false, "llm_analysis": {"is_bug": true, "is_runtime_error": true, "short_label": "unused_tensorflow_import_and_function", "bug_description": "buggy_code 中导入了 tensorflow 库并定义了 raft_flow_to_apply_v2 函数，但该函数在代码中从未被调用，导致不必要的依赖和潜在的导入错误。fixed_code 移除了未使用的 tensorflow 导入和函数，简化了代码并避免了潜在的运行时导入错误。"}}}
{"id": "bugfix030", "buggy_file": "data/raw_new2/bugfix030_buggy.py", "fixed_file": "data/raw_new2/bugfix030_fixed.py", "meta": {"repo": "etsy/logster", "file": "logster/logster_helper.py", "buggy_sha": "c5d703e8f571d68e14d9c40f1fa28df1bc550678", "fixed_sha": "4290c3ecb452bd2e32473c425cb76906311897a2", "commit_message": "Fix testing on python 2.5", "changes": 6, "patch": "@@ -89,8 +89,8 @@ def get_instance_id(self, instance_id = None):\n             try:\n                 conn = HTTPConnection(\"169.254.169.254\")\n                 conn.request(\"GET\", \"/latest/meta-data/instance-id\")\n-            except Exception as e:\n-                raise CloudWatchException(\"Can't connect Amazon meta data server to get InstanceID : (%s)\" % e.message)\n+            except Exception:\n+                raise CloudWatchException(\"Can't connect Amazon meta data server to get InstanceID : (%s)\")\n \n             self.instance_id = conn.getresponse().read()\n         \n@@ -139,7 +139,7 @@ def put_data(self):\n         try:\n             conn = HTTPConnection(self.base_url)\n             conn.request(\"GET\", signedURL)\n-        except Exception as e:\n+        except Exception:\n             raise CloudWatchException(\"Can't connect Amazon CloudWatch server\") \n         res = conn.getresponse()\n ", "syntax_error": false, "llm_analysis": {"is_bug": true, "is_runtime_error": true, "short_label": "exception_message_format_error", "bug_description": "在buggy_code中，当捕获到异常时，尝试使用e.message来格式化异常消息，但在Python 3中，Exception对象没有message属性，这会导致AttributeError。fixed_code移除了对e.message的引用，避免了运行时错误。"}}}
{"id": "bugfix031", "buggy_file": "data/raw_new2/bugfix031_buggy.py", "fixed_file": "data/raw_new2/bugfix031_fixed.py", "meta": {"repo": "upbit/pixivpy", "file": "pixivpy3/utils.py", "buggy_sha": "ecd53135f5d5f085914dd0d380be89f6f7395449", "fixed_sha": "565548aac5de20afa54afc096645d75969dc3061", "commit_message": "Merge pull request #394 from upbit/mock_tests\n\nFix workflow, use pytest (mock JSON data) replace demp.py checks", "changes": 8, "patch": "@@ -1,9 +1,13 @@\n from __future__ import annotations\n \n+import typing\n from typing import Any, Dict, Optional\n \n import requests\n-from requests.structures import CaseInsensitiveDict\n+\n+if typing.TYPE_CHECKING:\n+    from requests.structures import CaseInsensitiveDict\n+\n \n # from typeguard import typechecked\n \n@@ -21,7 +25,7 @@ def __init__(\n         reason: str,\n         header: dict[str, Any] | CaseInsensitiveDict[Any] | None = None,\n         body: str | None = None,\n-    ):\n+    ) -> None:\n         self.reason = str(reason)\n         self.header = header\n         self.body = body", "syntax_error": false, "llm_analysis": {"is_bug": true, "is_runtime_error": true, "short_label": "circular_import_fix", "bug_description": "buggy_code 中存在循环导入问题，直接导入 CaseInsensitiveDict 会导致在运行时出现 ImportError。fixed_code 通过使用 typing.TYPE_CHECKING 和条件导入，将导入移到类型检查块中，避免了循环导入导致的运行时错误。"}}}
{"id": "bugfix032", "buggy_file": "data/raw_new2/bugfix032_buggy.py", "fixed_file": "data/raw_new2/bugfix032_fixed.py", "meta": {"repo": "upbit/pixivpy", "file": "example_bypass_sni.py", "buggy_sha": "d144b42947a2de667e4b01b0c4f3d37e26664de1", "fixed_sha": "e3fd59064e74e86ed0302fbb92ead3b85f540208", "commit_message": "fix: Fix SniAPI host issue, upgrade requests to 2.31.0", "changes": 3, "patch": "@@ -13,7 +13,8 @@\n \n def main():\n     api = ByPassSniApi()  # Same as AppPixivAPI, but bypass the GFW\n-    api.require_appapi_hosts()\n+    # api.require_appapi_hosts()\n+    api.require_appapi_hosts(hostname=\"public-api.secure.pixiv.net\")\n     # api.set_additional_headers({'Accept-Language':'en-US'})\n     api.set_accept_language(\"en-us\")\n ", "syntax_error": false, "llm_analysis": {"is_bug": true, "is_runtime_error": true, "short_label": "missing_hostname_parameter", "bug_description": "buggy_code 中调用 api.require_appapi_hosts() 时缺少必要的 hostname 参数，这会导致 API 调用失败或连接错误。fixed_code 通过添加 hostname=\"public-api.secure.pixiv.net\" 参数来正确配置 API 主机地址，确保程序能够正常连接到 Pixiv 服务。"}}}
{"id": "bugfix033", "buggy_file": "data/raw_new2/bugfix033_buggy.py", "fixed_file": "data/raw_new2/bugfix033_fixed.py", "meta": {"repo": "alfiopuglisi/guietta", "file": "setup.py", "buggy_sha": "604e8ddb0a72e4c39c526af46675c6b868475784", "fixed_sha": "9f472773771842fd7abf59a4b09e93ae585f4d23", "commit_message": "Merge pull request #43 from chinoll/master\n\nFix an exception thrown when the language of Windows is set to Chinese", "changes": 2, "patch": "@@ -17,7 +17,7 @@\n # Load the package's __version__.py module as a dictionary.\n here = os.path.abspath(os.path.dirname(__file__))\n about = {}\n-with open(os.path.join(here, NAME, '__version__.py')) as f:\n+with open(os.path.join(here, NAME, '__version__.py'),encoding='utf-8') as f:\n     exec(f.read(), about)\n \n ", "syntax_error": false, "llm_analysis": {"is_bug": true, "is_runtime_error": true, "short_label": "missing_encoding_for_file_open", "bug_description": "buggy_code 在读取 __version__.py 文件时没有指定编码，当文件包含非ASCII字符时会导致 UnicodeDecodeError。fixed_code 添加了 encoding='utf-8' 参数来确保正确读取UTF-8编码的文件。"}}}
{"id": "bugfix034", "buggy_file": "data/raw_new2/bugfix034_buggy.py", "fixed_file": "data/raw_new2/bugfix034_fixed.py", "meta": {"repo": "kittinan/spotify-github-profile", "file": "util/firestore.py", "buggy_sha": "3548a904a48e83cbda14ae83b44c6e7d80b709b8", "fixed_sha": "7c46911bab79d0cde078cf7649adb68dfa201e7f", "commit_message": "Add giithub CI (#116)\n\n* Add unittest API view\n\n* Update README about run test\n\n* Ignore some files & folder\n\n* Setup pytest\n\n* Add dependencies about unittest uspport flask framework\n\n* Fixed handle edge case\n\n* Setup CI workflow\n\n* Add unittest API view sg\n\n* Fix for run test with mock", "changes": 7, "patch": "@@ -8,8 +8,15 @@\n \n \n def get_firestore_db():\n+    # In testing environment, return a mock client\n+    if os.getenv(\"TESTING\") == \"true\":\n+        from unittest.mock import MagicMock\n+        return MagicMock()\n+    \n     if not firebase_admin._apps:\n         firebase_config = os.getenv(\"FIREBASE\")\n+        if firebase_config is None:\n+            raise ValueError(\"FIREBASE environment variable is not set. Please set the Firebase configuration.\")\n         firebase_dict = json.loads(b64decode(firebase_config))\n \n         cred = credentials.Certificate(firebase_dict)", "syntax_error": false, "llm_analysis": {"is_bug": true, "is_runtime_error": true, "short_label": "missing_environment_variable_validation", "bug_description": "buggy_code 没有检查 FIREBASE 环境变量是否设置，当环境变量未设置时，json.loads(b64decode(None)) 会抛出异常。fixed_code 添加了环境变量检查，如果未设置则抛出明确的 ValueError，并增加了测试环境的 mock 支持。"}}}
{"id": "bugfix035", "buggy_file": "data/raw_new2/bugfix035_buggy.py", "fixed_file": "data/raw_new2/bugfix035_fixed.py", "meta": {"repo": "kittinan/spotify-github-profile", "file": "api/theme_dev.py", "buggy_sha": "d015c3a10455a6e263db115e025a4e7e1fbcbd77", "fixed_sha": "16c1f5d40b157b143d95d51e55ff6e351541fc77", "commit_message": "fix: theme dev", "changes": 11, "patch": "@@ -6,7 +6,7 @@\n app = Flask(__name__)\n \n \n-def make_svg(artist_name, song_name, img, is_now_playing, cover_image, theme):\n+def make_svg(artist_name, song_name, img, is_now_playing, cover_image, theme, bar_color):\n     height = 445 if cover_image else 145\n     num_bar = 75\n \n@@ -29,12 +29,10 @@ def make_svg(artist_name, song_name, img, is_now_playing, cover_image, theme):\n         \"song_name\": song_name,\n         \"img\": img,\n         \"cover_image\": cover_image,\n+        \"bar_color\": bar_color,\n     }\n \n-    if theme != 'default':\n-      return render_template(f\"spotify.{theme}.html.j2\", **rendered_data)\n-    else:\n-      return render_template(\"spotify.html.j2\", **rendered_data)\n+    return render_template(f\"spotify.{theme}.html.j2\", **rendered_data)\n \n \n @app.route(\"/\", defaults={\"path\": \"\"})\n@@ -49,8 +47,9 @@ def catch_all(path):\n     is_now_playing = True\n     cover_image = True\n     theme = 'default'\n+    bar_color = '53b14f'\n \n-    svg = make_svg(artist_name, song_name, img, is_now_playing, cover_image, theme)\n+    svg = make_svg(artist_name, song_name, img, is_now_playing, cover_image, theme, bar_color)\n \n     resp = Response(svg, mimetype=\"image/svg+xml\")\n     resp.headers[\"Cache-Control\"] = \"s-maxage=1\"", "syntax_error": false, "llm_analysis": {"is_bug": true, "is_runtime_error": true, "short_label": "missing_bar_color_parameter", "bug_description": "buggy_code 中的 make_svg 函数缺少 bar_color 参数，但模板渲染时使用了该变量，导致当 theme 不为 'default' 时，模板中引用 bar_color 会引发 NameError 异常。fixed_code 添加了 bar_color 参数并移除了条件渲染逻辑，统一使用 spotify.{theme}.html.j2 模板。"}}}
{"id": "bugfix036", "buggy_file": "data/raw_new2/bugfix036_buggy.py", "fixed_file": "data/raw_new2/bugfix036_fixed.py", "meta": {"repo": "noamgat/lm-format-enforcer", "file": "tests/common.py", "buggy_sha": "566a20ac4bff7371d31d329731c642f0831c3ffe", "fixed_sha": "ec70f018f28c14fa07f11ced2fe4cea8790e63e1", "commit_message": "v0.11.1: Bitmasks (#168)\n\n* Minor improvements for vLLM V1 engine support\n\n* Updating github action version\n\n* WIP: bitmasks\n\n* Supporting bitmasks as internal structures in token enforcer\n\n* bitmask related fixes and some performance improvments\n\n* Added documentation before next release\n\n* Enabling vLLM to set use_bitmask=True", "changes": 16, "patch": "@@ -10,7 +10,7 @@\n             \n \n _tokenizer: Optional[PreTrainedTokenizerBase] = None\n-_tokenizer_data: Optional[TokenEnforcerTokenizerData] = None\n+_tokenizer_data: dict[bool, TokenEnforcerTokenizerData] = {}\n \n \n class CharacterNotAllowedException(LMFormatEnforcerException):\n@@ -53,8 +53,11 @@ def assert_parser_with_string_token_enforcer(string: str, parser: CharacterLevel\n             logging.basicConfig(level=logging.INFO)\n             logging.warning(\"Encountered out-of-tokenizer character, LMFE does not deal with this well\")\n     \n-    if _tokenizer_data is None:\n-        _tokenizer_data = build_token_enforcer_tokenizer_data(_tokenizer)\n+    use_bitmask = len(string) % 2 == 1  # Consistent per test, varied per entire testset\n+\n+    if use_bitmask not in _tokenizer_data:\n+        _tokenizer_data[use_bitmask] = build_token_enforcer_tokenizer_data(_tokenizer, use_bitmask, len(_tokenizer))\n+    tokenizer_data = _tokenizer_data[use_bitmask]\n         \n     prompt = \"This is my question:\\n\\n\"\n     initial_token_array = _tokenizer.encode(prompt)\n@@ -67,7 +70,8 @@ def assert_parser_with_string_token_enforcer(string: str, parser: CharacterLevel\n     if not eos_token_id:\n         raise ValueError(f\"Tokenizer does not have {'an EOS token' if eos_token_id is None else 'EOS tokens'}\")\n     \n-    token_enforcer = TokenEnforcer(_tokenizer_data, parser)\n+    \n+    token_enforcer = TokenEnforcer(tokenizer_data, parser)\n     # The token enforcer is stateful - it keeps track of the parsing state as tokens arrive on a token by token basis.\n     # We simulate a language model that \"chooses\" the next token in the encoded sequence, and check that it is in the\n     # allowed list at every timestep.\n@@ -80,7 +84,7 @@ def assert_parser_with_string_token_enforcer(string: str, parser: CharacterLevel\n         allowed_tokens = token_enforcer.get_allowed_tokens(prefix)\n         if prefix_length < len(target_token_array):\n             next_token = target_token_array[prefix_length]\n-            if next_token not in allowed_tokens:\n+            if not allowed_tokens.is_token_allowed(next_token):\n                 if expect_success:\n                     decoded_before = _tokenizer.decode(prefix, skip_special_tokens=True)\n                     decoded_after = _tokenizer.decode(prefix + [next_token], skip_special_tokens=True)\n@@ -91,7 +95,7 @@ def assert_parser_with_string_token_enforcer(string: str, parser: CharacterLevel\n                     return  # Test success\n         else:\n             # Reached the end of the sequence, check that ending state matches expected ending state\n-            can_end = any(token in allowed_tokens for token in (eos_token_id if isinstance(eos_token_id, list) else [eos_token_id]))\n+            can_end = any(allowed_tokens.is_token_allowed(token) for token in (eos_token_id if isinstance(eos_token_id, list) else [eos_token_id]))\n             if can_end and not expect_success:\n                 raise ValueError(\"Parser succeeded when it should have failed\")\n             if not can_end and expect_success:", "syntax_error": false, "llm_analysis": {"is_bug": true, "is_runtime_error": true, "short_label": "incorrect_token_validation_logic", "bug_description": "buggy_code 错误地使用 `next_token not in allowed_tokens` 来检查令牌是否被允许，但 `allowed_tokens` 是一个对象而不是集合。fixed_code 通过调用 `allowed_tokens.is_token_allowed(next_token)` 来正确验证令牌，并修复了 `_tokenizer_data` 的缓存机制以支持不同的 bitmask 配置。"}}}
{"id": "bugfix037", "buggy_file": "data/raw_new2/bugfix037_buggy.py", "fixed_file": "data/raw_new2/bugfix037_fixed.py", "meta": {"repo": "noamgat/lm-format-enforcer", "file": "tests/test_tokenizercaching.py", "buggy_sha": "566a20ac4bff7371d31d329731c642f0831c3ffe", "fixed_sha": "ec70f018f28c14fa07f11ced2fe4cea8790e63e1", "commit_message": "v0.11.1: Bitmasks (#168)\n\n* Minor improvements for vLLM V1 engine support\n\n* Updating github action version\n\n* WIP: bitmasks\n\n* Supporting bitmasks as internal structures in token enforcer\n\n* bitmask related fixes and some performance improvments\n\n* Added documentation before next release\n\n* Enabling vLLM to set use_bitmask=True", "changes": 4, "patch": "@@ -3,7 +3,7 @@\n \n def test_json_freetext_cache():\n     token_to_str = {}\n-    cache = JsonFreetextTokenCache()\n+    cache = JsonFreetextTokenCache(False, 0)\n     test_length = 500\n     letters = \"abcde\"\n     num_letters = len(letters)\n@@ -19,7 +19,7 @@ def _register_token(token_str: str):\n     cache.freeze()\n \n     def _assert_allowed_tokens(_min_remaining, _max_length, _num_expected_tokens):\n-        allowed_tokens = cache.lookup_allowed_tokens(_min_remaining, _max_length)\n+        allowed_tokens = cache.lookup_allowed_tokens(_min_remaining, _max_length).allowed_tokens\n         if len(allowed_tokens) != _num_expected_tokens:\n             allowed_token_strs = \"|\".join(token_to_str[token_idx] for token_idx in allowed_tokens)\n             raise Exception(f\"Min={_min_remaining}, Max={_max_length}, Expected {_num_expected_tokens}, got {len(allowed_tokens)} : {allowed_token_strs}\")", "syntax_error": false, "llm_analysis": {"is_bug": true, "is_runtime_error": true, "short_label": "attribute_error_in_lookup_allowed_tokens", "bug_description": "buggy_code 中 cache.lookup_allowed_tokens() 返回的是一个对象而不是直接的 token 列表，直接使用会导致 AttributeError。fixed_code 通过添加 .allowed_tokens 属性访问来正确获取 token 列表。"}}}
{"id": "bugfix038", "buggy_file": "data/raw_new2/bugfix038_buggy.py", "fixed_file": "data/raw_new2/bugfix038_fixed.py", "meta": {"repo": "initstring/cloud_enum", "file": "enum_tools/aws_checks.py", "buggy_sha": "3f44476d1493f62d6e0ac419984fdcafe4b773e5", "fixed_sha": "0e54b48c7d9567e55b554e4065982d722de640e7", "commit_message": "Small gift :) nameserverfile + more Azure objects + fuzzing list improved (#65)\n\n* PR with multiple additions :)\r\n\r\n* Removed debugging thingies\r\n\r\n* Fix var not defined\r\n\r\n* bug fix brute_force_containers", "changes": 6, "patch": "@@ -98,7 +98,7 @@ def check_s3_buckets(names, threads):\n     utils.stop_timer(start_time)\n \n \n-def check_awsapps(names, threads, nameserver):\n+def check_awsapps(names, threads, nameserver, nameserverfile=False):\n     \"\"\"\n     Checks for existence of AWS Apps\n     (ie. WorkDocs, WorkMail, Connect, etc.)\n@@ -122,7 +122,7 @@ def check_awsapps(names, threads, nameserver):\n \n     # AWS Apps use DNS sub-domains. First, see which are valid.\n     valid_names = utils.fast_dns_lookup(candidates, nameserver,\n-                                        threads=threads)\n+                                        nameserverfile, threads=threads)\n \n     for name in valid_names:\n         data['target'] = f'https://{name}'\n@@ -143,4 +143,4 @@ def run_all(names, args):\n     # if not regions:\n     #    regions = AWS_REGIONS\n     check_s3_buckets(names, args.threads)\n-    check_awsapps(names, args.threads, args.nameserver)\n+    check_awsapps(names, args.threads, args.nameserver, args.nameserverfile)", "syntax_error": false, "llm_analysis": {"is_bug": true, "is_runtime_error": true, "short_label": "missing_nameserverfile_argument", "bug_description": "buggy_code 在调用 utils.fast_dns_lookup 时缺少 nameserverfile 参数，这会导致函数调用时参数不匹配而抛出 TypeError 异常。fixed_code 在 check_awsapps 函数中添加了 nameserverfile 参数并传递到 DNS 查找函数中。"}}}
{"id": "bugfix039", "buggy_file": "data/raw_new2/bugfix039_buggy.py", "fixed_file": "data/raw_new2/bugfix039_fixed.py", "meta": {"repo": "initstring/cloud_enum", "file": "cloud_enum.py", "buggy_sha": "ecf4ca608a8630f13823ba089f6e3a4b3c169877", "fixed_sha": "e9f4a245cff0a05380311f194e8da65226355b35", "commit_message": "Log formats and general improvements (#47)\n\n- Added logging formats (text, csv, json)\r\n- Linting\r\n- Additional exception handling\r\n- Better GCP AppEngine enumeration\r\n- Handling of rare DNS issues", "changes": 46, "patch": "@@ -26,7 +26,6 @@\n \n '''\n \n-LOGFILE = False\n \n def parse_arguments():\n     \"\"\"\n@@ -68,7 +67,11 @@ def parse_arguments():\n                         help='DNS server to use in brute-force.')\n \n     parser.add_argument('-l', '--logfile', type=str, action='store',\n-                        help='Will APPEND found items to specified file.')\n+                        help='Appends found items to specified file.')\n+    parser.add_argument('-f', '--format', type=str, action='store',\n+                        default='text',\n+                        help='Format for log file (text,json,csv)'\n+                             ' - default: text')\n \n     parser.add_argument('--disable-aws', action='store_true',\n                         help='Disable Amazon checks.')\n@@ -86,8 +89,7 @@ def parse_arguments():\n \n     # Ensure mutations file is readable\n     if not os.access(args.mutations, os.R_OK):\n-        print(\"[!] Cannot access mutations file: {}\"\n-              .format(args.mutations))\n+        print(f\"[!] Cannot access mutations file: {args.mutations}\")\n         sys.exit()\n \n     # Ensure brute file is readable\n@@ -102,7 +104,7 @@ def parse_arguments():\n             sys.exit()\n \n         # Parse keywords from input file\n-        with open(args.keyfile) as infile:\n+        with open(args.keyfile, encoding='utf-8') as infile:\n             args.keyword = [keyword.strip() for keyword in infile]\n \n     # Ensure log file is writeable\n@@ -121,23 +123,29 @@ def parse_arguments():\n             print(\"[!] Cannot write to log file, exiting\")\n             sys.exit()\n \n+        # Set up logging format\n+        if args.format not in ('text', 'json', 'csv'):\n+            print(\"[!] Sorry! Allowed log formats: 'text', 'json', or 'csv'\")\n+            sys.exit()\n         # Set the global in the utils file, where logging needs to happen\n-        utils.init_logfile(args.logfile)\n+        utils.init_logfile(args.logfile, args.format)\n \n     return args\n \n+\n def print_status(args):\n     \"\"\"\n     Print a short pre-run status message\n     \"\"\"\n-    print(\"Keywords:    {}\".format(', '.join(args.keyword)))\n+    print(f\"Keywords:    {', '.join(args.keyword)}\")\n     if args.quickscan:\n         print(\"Mutations:   NONE! (Using quickscan)\")\n     else:\n-        print(\"Mutations:   {}\".format(args.mutations))\n-    print(\"Brute-list:  {}\".format(args.brute))\n+        print(f\"Mutations:   {args.mutations}\")\n+    print(f\"Brute-list:  {args.brute}\")\n     print(\"\")\n \n+\n def check_windows():\n     \"\"\"\n     Fixes pretty color printing for Windows users. Keeping out of\n@@ -151,16 +159,18 @@ def check_windows():\n             print(\"[!] Yo, Windows user - if you want pretty colors, you can\"\n                   \" install the colorama python package.\")\n \n+\n def read_mutations(mutations_file):\n     \"\"\"\n     Read mutations file into memory for processing.\n     \"\"\"\n     with open(mutations_file, encoding=\"utf8\", errors=\"ignore\") as infile:\n         mutations = infile.read().splitlines()\n \n-    print(\"[+] Mutations list imported: {} items\".format(len(mutations)))\n+    print(f\"[+] Mutations list imported: {len(mutations)} items\")\n     return mutations\n \n+\n def clean_text(text):\n     \"\"\"\n     Clean text to be RFC compliant for hostnames / DNS\n@@ -171,6 +181,7 @@ def clean_text(text):\n \n     return text_clean\n \n+\n def build_names(base_list, mutations):\n     \"\"\"\n     Combine base and mutations for processing by individual modules.\n@@ -189,19 +200,20 @@ def build_names(base_list, mutations):\n             mutation = clean_text(mutation)\n \n             # Then, do appends\n-            names.append(\"{}{}\".format(base, mutation))\n-            names.append(\"{}.{}\".format(base, mutation))\n-            names.append(\"{}-{}\".format(base, mutation))\n+            names.append(f\"{base}{mutation}\")\n+            names.append(f\"{base}.{mutation}\")\n+            names.append(f\"{base}-{mutation}\")\n \n             # Then, do prepends\n-            names.append(\"{}{}\".format(mutation, base))\n-            names.append(\"{}.{}\".format(mutation, base))\n-            names.append(\"{}-{}\".format(mutation, base))\n+            names.append(f\"{mutation}{base}\")\n+            names.append(f\"{mutation}.{base}\")\n+            names.append(f\"{mutation}-{base}\")\n \n-    print(\"[+] Mutated results: {} items\".format(len(names)))\n+    print(f\"[+] Mutated results: {len(names)} items\")\n \n     return names\n \n+\n def main():\n     \"\"\"\n     Main program function.", "syntax_error": false, "llm_analysis": {"is_bug": true, "is_runtime_error": true, "short_label": "encoding_error_on_keyword_file", "bug_description": "在buggy_code中，当使用-kf参数读取关键词文件时，没有指定文件编码，如果文件包含非ASCII字符会导致UnicodeDecodeError。fixed_code通过添加encoding='utf-8'参数来正确处理UTF-8编码的文件。"}}}
{"id": "bugfix040", "buggy_file": "data/raw_new2/bugfix040_buggy.py", "fixed_file": "data/raw_new2/bugfix040_fixed.py", "meta": {"repo": "initstring/cloud_enum", "file": "enum_tools/azure_checks.py", "buggy_sha": "ecf4ca608a8630f13823ba089f6e3a4b3c169877", "fixed_sha": "e9f4a245cff0a05380311f194e8da65226355b35", "commit_message": "Log formats and general improvements (#47)\n\n- Added logging formats (text, csv, json)\r\n- Linting\r\n- Additional exception handling\r\n- Better GCP AppEngine enumeration\r\n- Handling of rare DNS issues", "changes": 115, "patch": "@@ -31,24 +31,34 @@ def print_account_response(reply):\n     This function is passed into the class object so we can view results\n     in real-time.\n     \"\"\"\n+    data = {'platform': 'azure', 'msg': '', 'target': '', 'access': ''}\n+\n     if reply.status_code == 404:\n         pass\n     elif 'Server failed to authenticate the request' in reply.reason:\n-        utils.printc(\"    Auth-Only Storage Account: {}\\n\"\n-                     .format(reply.url), 'red')\n+        data['msg'] = 'Auth-Only Storage Account'\n+        data['target'] = reply.url\n+        data['access'] = 'protected'\n+        utils.fmt_output(data)\n     elif 'The specified account is disabled' in reply.reason:\n-        utils.printc(\"    Disabled Storage Account: {}\\n\"\n-                     .format(reply.url), 'red')\n+        data['msg'] = 'Disabled Storage Account'\n+        data['target'] = reply.url\n+        data['access'] = 'disabled'\n+        utils.fmt_output(data)\n     elif 'Value for one of the query' in reply.reason:\n-        utils.printc(\"    HTTP-OK Storage Account: {}\\n\"\n-                     .format(reply.url), 'orange')\n+        data['msg'] = 'HTTP-OK Storage Account'\n+        data['target'] = reply.url\n+        data['access'] = 'public'\n+        utils.fmt_output(data)\n     elif 'The account being accessed' in reply.reason:\n-        utils.printc(\"    HTTPS-Only Storage Account: {}\\n\"\n-                     .format(reply.url), 'orange')\n+        data['msg'] = 'HTTPS-Only Storage Account'\n+        data['target'] = reply.url\n+        data['access'] = 'public'\n+        utils.fmt_output(data)\n     else:\n-        print(\"    Unknown status codes being received from {}:\\n\"\n-              \"       {}: {}\"\n-              .format(reply.url, reply.status_code, reply.reason))\n+        print(\"    Unknown status codes being received from {reply.url}:\\n\"\n+              \"       {reply.status_code}: {reply.reason}\")\n+\n \n def check_storage_accounts(names, threads, nameserver):\n     \"\"\"\n@@ -71,7 +81,7 @@ def check_storage_accounts(names, threads, nameserver):\n     regex = re.compile('[^a-zA-Z0-9]')\n     for name in names:\n         if not re.search(regex, name):\n-            candidates.append('{}.{}'.format(name, BLOB_URL))\n+            candidates.append(f'{name}.{BLOB_URL}')\n \n     # Azure Storage Accounts use DNS sub-domains. First, see which are valid.\n     valid_names = utils.fast_dns_lookup(candidates, nameserver,\n@@ -88,13 +98,16 @@ def check_storage_accounts(names, threads, nameserver):\n     # de-dupe the results and return\n     return list(set(valid_names))\n \n+\n def print_container_response(reply):\n     \"\"\"\n     Parses the HTTP reply of a brute-force attempt\n \n     This function is passed into the class object so we can view results\n     in real-time.\n     \"\"\"\n+    data = {'platform': 'azure', 'msg': '', 'target': '', 'access': ''}\n+\n     # Stop brute forcing disabled accounts\n     if 'The specified account is disabled' in reply.reason:\n         print(\"    [!] Breaking out early, account disabled.\")\n@@ -117,17 +130,21 @@ def print_container_response(reply):\n     if reply.status_code == 404:\n         pass\n     elif reply.status_code == 200:\n-        utils.printc(\"    OPEN AZURE CONTAINER: {}\\n\"\n-                     .format(reply.url), 'green')\n+        data['msg'] = 'OPEN AZURE CONTAINER'\n+        data['target'] = reply.url\n+        data['access'] = 'public'\n+        utils.fmt_output(data)\n         utils.list_bucket_contents(reply.url)\n     elif 'One of the request inputs is out of range' in reply.reason:\n         pass\n     elif 'The request URI is invalid' in reply.reason:\n         pass\n     else:\n-        print(\"    Unknown status codes being received from {}:\\n\"\n-              \"       {}: {}\"\n-              .format(reply.url, reply.status_code, reply.reason))\n+        print(f\"    Unknown status codes being received from {reply.url}:\\n\"\n+              \"       {reply.status_code}: {reply.reason}\")\n+\n+    return None\n+\n \n def brute_force_containers(storage_accounts, brute_list, threads):\n     \"\"\"\n@@ -140,37 +157,37 @@ def brute_force_containers(storage_accounts, brute_list, threads):\n     # We have a list of valid DNS names that might not be worth scraping,\n     # such as disabled accounts or authentication required. Let's quickly\n     # weed those out.\n-    print(\"[*] Checking {} accounts for status before brute-forcing\"\n-          .format(len(storage_accounts)))\n+    print(f\"[*] Checking {len(storage_accounts)} accounts for status before brute-forcing\")\n     valid_accounts = []\n     for account in storage_accounts:\n-        reply = requests.get('https://{}/'.format(account))\n-        if 'Server failed to authenticate the request' in reply.reason:\n-            storage_accounts.remove(account)\n-        elif 'The specified account is disabled' in reply.reason:\n-            storage_accounts.remove(account)\n-        else:\n-            valid_accounts.append(account)\n+        try:\n+            reply = requests.get(f'https://{account}/')\n+            if 'Server failed to authenticate the request' in reply.reason:\n+                storage_accounts.remove(account)\n+            elif 'The specified account is disabled' in reply.reason:\n+                storage_accounts.remove(account)\n+            else:\n+                valid_accounts.append(account)\n+        except requests.exceptions.ConnectionError as error_msg:\n+            print(f\"    [!] Connection error on {url}:\")\n+            print(error_msg)\n \n     # Read the brute force file into memory\n     clean_names = utils.get_brute(brute_list, mini=3)\n \n     # Start a counter to report on elapsed time\n     start_time = utils.start_timer()\n \n-    print(\"[*] Brute-forcing container names in {} storage accounts\"\n-          .format(len(valid_accounts)))\n+    print(f\"[*] Brute-forcing container names in {len(valid_accounts)} storage accounts\")\n     for account in valid_accounts:\n-        print(\"[*] Brute-forcing {} container names in {}\"\n-              .format(len(clean_names), account))\n+        print(f\"[*] Brute-forcing {len(clean_names)} container names in {account}\")\n \n         # Initialize the list of correctly formatted urls\n         candidates = []\n \n         # Take each mutated keyword and craft a url with correct format\n         for name in clean_names:\n-            candidates.append('{}/{}/?restype=container&comp=list'\n-                              .format(account, name))\n+            candidates.append(f'{account}/{name}/?restype=container&comp=list')\n \n         # Send the valid names to the batch HTTP processor\n         utils.get_url_batch(candidates, use_ssl=True,\n@@ -180,13 +197,19 @@ def brute_force_containers(storage_accounts, brute_list, threads):\n     # Stop the timer\n     utils.stop_timer(start_time)\n \n+\n def print_website_response(hostname):\n     \"\"\"\n     This function is passed into the DNS brute force as a callback,\n     so we can get real-time results.\n     \"\"\"\n-    utils.printc(\"    Registered Azure Website DNS Name: {}\\n\"\n-                 .format(hostname), 'green')\n+    data = {'platform': 'azure', 'msg': '', 'target': '', 'access': ''}\n+\n+    data['msg'] = 'Registered Azure Website DNS Name'\n+    data['target'] = hostname\n+    data['access'] = 'public'\n+    utils.fmt_output(data)\n+\n \n def check_azure_websites(names, nameserver, threads):\n     \"\"\"\n@@ -208,13 +231,19 @@ def check_azure_websites(names, nameserver, threads):\n     # Stop the timer\n     utils.stop_timer(start_time)\n \n+\n def print_database_response(hostname):\n     \"\"\"\n     This function is passed into the DNS brute force as a callback,\n     so we can get real-time results.\n     \"\"\"\n-    utils.printc(\"    Registered Azure Database DNS Name: {}\\n\"\n-                 .format(hostname), 'green')\n+    data = {'platform': 'azure', 'msg': '', 'target': '', 'access': ''}\n+\n+    data['msg'] = 'Registered Azure Database DNS Name'\n+    data['target'] = hostname\n+    data['access'] = 'public'\n+    utils.fmt_output(data)\n+\n \n def check_azure_databases(names, nameserver, threads):\n     \"\"\"\n@@ -236,13 +265,19 @@ def check_azure_databases(names, nameserver, threads):\n     # Stop the timer\n     utils.stop_timer(start_time)\n \n+\n def print_vm_response(hostname):\n     \"\"\"\n     This function is passed into the DNS brute force as a callback,\n     so we can get real-time results.\n     \"\"\"\n-    utils.printc(\"    Registered Azure Virtual Machine DNS Name: {}\\n\"\n-                 .format(hostname), 'green')\n+    data = {'platform': 'azure', 'msg': '', 'target': '', 'access': ''}\n+\n+    data['msg'] = 'Registered Azure Virtual Machine DNS Name'\n+    data['target'] = hostname\n+    data['access'] = 'public'\n+    utils.fmt_output(data)\n+\n \n def check_azure_vms(names, nameserver, threads):\n     \"\"\"\n@@ -256,8 +291,7 @@ def check_azure_vms(names, nameserver, threads):\n     # Pull the regions from a config file\n     regions = azure_regions.REGIONS\n \n-    print(\"[*] Testing across {} regions defined in the config file\"\n-          .format(len(regions)))\n+    print(f\"[*] Testing across {len(regions)} regions defined in the config file\")\n \n     for region in regions:\n \n@@ -272,6 +306,7 @@ def check_azure_vms(names, nameserver, threads):\n     # Stop the timer\n     utils.stop_timer(start_time)\n \n+\n def run_all(names, args):\n     \"\"\"\n     Function is called by main program", "syntax_error": false, "llm_analysis": {"is_bug": true, "is_runtime_error": true, "short_label": "connection_error_handling_missing", "bug_description": "在brute_force_containers函数中，buggy版本没有处理网络连接错误。当requests.get()遇到连接问题时（如DNS解析失败、网络超时等），会抛出requests.exceptions.ConnectionError异常，导致程序崩溃。fixed版本添加了try-except块来捕获并处理这些连接错误，使程序能够继续运行而不是崩溃。"}}}
{"id": "bugfix041", "buggy_file": "data/raw_new2/bugfix041_buggy.py", "fixed_file": "data/raw_new2/bugfix041_fixed.py", "meta": {"repo": "initstring/cloud_enum", "file": "enum_tools/utils.py", "buggy_sha": "ecf4ca608a8630f13823ba089f6e3a4b3c169877", "fixed_sha": "e9f4a245cff0a05380311f194e8da65226355b35", "commit_message": "Log formats and general improvements (#47)\n\n- Added logging formats (text, csv, json)\r\n- Linting\r\n- Additional exception handling\r\n- Better GCP AppEngine enumeration\r\n- Handling of rare DNS issues", "changes": 94, "patch": "@@ -6,6 +6,8 @@\n import sys\n import datetime\n import re\n+import csv\n+import json\n from multiprocessing.dummy import Pool as ThreadPool\n from functools import partial\n try:\n@@ -20,19 +22,24 @@\n     sys.exit()\n \n LOGFILE = False\n+LOGFILE_FMT = ''\n \n-def init_logfile(logfile):\n+\n+def init_logfile(logfile, fmt):\n     \"\"\"\n     Initialize the global logfile if specified as a user-supplied argument\n     \"\"\"\n     if logfile:\n         global LOGFILE\n         LOGFILE = logfile\n \n+        global LOGFILE_FMT\n+        LOGFILE_FMT = fmt\n+\n         now = datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n-        with open(logfile, 'a') as log_writer:\n-            log_writer.write(\"\\n\\n#### CLOUD_ENUM {} ####\\n\"\n-                             .format(now))\n+        with open(logfile, 'a', encoding='utf-8') as log_writer:\n+            log_writer.write(f\"\\n\\n#### CLOUD_ENUM {now} ####\\n\")\n+\n \n def get_url_batch(url_list, use_ssl=False, callback='', threads=5, redir=True):\n     \"\"\"\n@@ -80,11 +87,11 @@ def get_url_batch(url_list, use_ssl=False, callback='', threads=5, redir=True):\n                 # hanging forever with no exception raised.\n                 batch_results[url] = batch_pending[url].result(timeout=30)\n             except requests.exceptions.ConnectionError as error_msg:\n-                print(\"    [!] Connection error on {}:\".format(url))\n+                print(f\"    [!] Connection error on {url}:\")\n                 print(error_msg)\n             except TimeoutError:\n-                print(\"    [!] Timeout on {}. Investigate if there are\"\n-                      \" many of these\".format(url))\n+                print(f\"    [!] Timeout on {url}. Investigate if there are\"\n+                      \" many of these\")\n \n         # Now, send all the results to the callback function for analysis\n         # We need a way to stop processing unnecessary brute-forces, so the\n@@ -97,13 +104,13 @@ def get_url_batch(url_list, use_ssl=False, callback='', threads=5, redir=True):\n         # Refresh a status message\n         tick['current'] += threads\n         sys.stdout.flush()\n-        sys.stdout.write(\"    {}/{} complete...\"\n-                         .format(tick['current'], tick['total']))\n+        sys.stdout.write(f\"    {tick['current']}/{tick['total']} complete...\")\n         sys.stdout.write('\\r')\n \n     # Clear the status message\n     sys.stdout.write('                            \\r')\n \n+\n def dns_lookup(nameserver, name):\n     \"\"\"\n     This function performs the actual DNS lookup when called in a threadpool\n@@ -119,9 +126,18 @@ def dns_lookup(nameserver, name):\n         return name\n     except dns.resolver.NXDOMAIN:\n         return ''\n+    except dns.resolver.NoNameservers as exc_text:\n+        print(\"    [!] Error querying nameservers! This could be a problem.\")\n+        print(\"    [!] If you're using a VPN, try setting --ns to your VPN's nameserver.\")\n+        print(\"    [!] Bailing because you need to fix this\")\n+        print(\"    [!] More Info:\")\n+        print(exc_text)\n+        return '-#BREAKOUT_DNS_ERROR#-'\n     except dns.exception.Timeout:\n-        print(\"    [!] DNS Timeout on {}. Investigate if there are many\"\n-              \" of these.\".format(name))\n+        print(f\"    [!] DNS Timeout on {name}. Investigate if there are many\"\n+              \" of these.\")\n+        return ''\n+\n \n def fast_dns_lookup(names, nameserver, callback='', threads=5):\n     \"\"\"\n@@ -131,7 +147,7 @@ def fast_dns_lookup(names, nameserver, callback='', threads=5):\n     current = 0\n     valid_names = []\n \n-    print(\"[*] Brute-forcing a list of {} possible DNS names\".format(total))\n+    print(f\"[*] Brute-forcing a list of {total} possible DNS names\")\n \n     # Break the url list into smaller lists based on thread size\n     queue = [names[x:x+threads] for x in range(0, len(names), threads)]\n@@ -148,6 +164,8 @@ def fast_dns_lookup(names, nameserver, callback='', threads=5):\n         # We should now have the batch of results back, process them.\n         for name in results:\n             if name:\n+                if name == '-#BREAKOUT_DNS_ERROR#-':\n+                    sys.exit()\n                 if callback:\n                     callback(name)\n                 valid_names.append(name)\n@@ -156,7 +174,7 @@ def fast_dns_lookup(names, nameserver, callback='', threads=5):\n \n         # Update the status message\n         sys.stdout.flush()\n-        sys.stdout.write(\"    {}/{} complete...\".format(current, total))\n+        sys.stdout.write(f\"    {current}/{total} complete...\")\n         sys.stdout.write('\\r')\n         pool.close()\n \n@@ -165,6 +183,7 @@ def fast_dns_lookup(names, nameserver, callback='', threads=5):\n \n     return valid_names\n \n+\n def list_bucket_contents(bucket):\n     \"\"\"\n     Provides a list of full URLs to each open bucket\n@@ -182,38 +201,41 @@ def list_bucket_contents(bucket):\n \n     # Format them to full URLs and print to console\n     if keys:\n-        printc(\"      FILES:\\n\", 'none')\n+        print(\"      FILES:\")\n         for key in keys:\n             url = bucket + key\n-            printc(\"      ->{}\\n\".format(url), 'none')\n+            print(f\"      ->{url}\")\n     else:\n-        printc(\"      ...empty bucket, so sad. :(\\n\", 'none')\n+        print(\"      ...empty bucket, so sad. :(\")\n+\n \n-def printc(text, color):\n+def fmt_output(data):\n     \"\"\"\n-    Prints colored text to screen\n+    Handles the output - printing and logging based on a specified format\n     \"\"\"\n-    # ANSI escape sequences\n-    green = '\\033[92m'\n-    orange = '\\033[33m'\n-    red = '\\033[31m'\n+    # ANSI escape sequences are set based on accessibility of target\n+    # (basically, how public it is))\n     bold = '\\033[1m'\n     end = '\\033[0m'\n+    if data['access'] == 'public':\n+        ansi = bold + '\\033[92m'  # green\n+    if data['access'] == 'protected':\n+        ansi = bold + '\\033[33m'  # orange\n+    if data['access'] == 'disabled':\n+        ansi = bold + '\\033[31m'  # red\n \n-    if color == 'orange':\n-        sys.stdout.write(bold + orange + text + end)\n-    if color == 'green':\n-        sys.stdout.write(bold + green + text + end)\n-    if color == 'red':\n-        sys.stdout.write(bold + red + text + end)\n-    if color == 'black':\n-        sys.stdout.write(bold + text + end)\n-    if color == 'none':\n-        sys.stdout.write(text)\n+    sys.stdout.write('  ' + ansi + data['msg'] + ': ' + data['target'] + end + '\\n')\n \n     if LOGFILE:\n-        with open(LOGFILE, 'a')  as log_writer:\n-            log_writer.write(text.lstrip())\n+        with open(LOGFILE, 'a', encoding='utf-8') as log_writer:\n+            if LOGFILE_FMT == 'text':\n+                log_writer.write(f'{data[\"msg\"]}: {data[\"target\"]}\\n')\n+            if LOGFILE_FMT == 'csv':\n+                writer = csv.DictWriter(log_writer, data.keys())\n+                writer.writerow(data)\n+            if LOGFILE_FMT == 'json':\n+                log_writer.write(json.dumps(data) + '\\n')\n+\n \n def get_brute(brute_file, mini=1, maxi=63, banned='[^a-z0-9_-]'):\n     \"\"\"\n@@ -235,6 +257,7 @@ def get_brute(brute_file, mini=1, maxi=63, banned='[^a-z0-9_-]'):\n \n     return clean_names\n \n+\n def start_timer():\n     \"\"\"\n     Starts a timer for functions in main module\n@@ -243,6 +266,7 @@ def start_timer():\n     start_time = time.time()\n     return start_time\n \n+\n def stop_timer(start_time):\n     \"\"\"\n     Stops timer and prints a status\n@@ -253,5 +277,5 @@ def stop_timer(start_time):\n \n     # Print some statistics\n     print(\"\")\n-    print(\" Elapsed time: {}\".format(formatted_time))\n+    print(f\" Elapsed time: {formatted_time}\")\n     print(\"\")", "syntax_error": false, "llm_analysis": {"is_bug": true, "is_runtime_error": true, "short_label": "missing_dns_exception_handling", "bug_description": "buggy_code 缺少对 dns.resolver.NoNameservers 异常的处理，当 DNS 查询遇到无可用名称服务器时会抛出未捕获的异常导致程序崩溃。fixed_code 添加了对该异常的捕获，打印错误信息并返回特殊标记，在 fast_dns_lookup 中检测到该标记时优雅退出程序。"}}}
{"id": "bugfix042", "buggy_file": "data/raw_new2/bugfix042_buggy.py", "fixed_file": "data/raw_new2/bugfix042_fixed.py", "meta": {"repo": "Xyntax/POC-T", "file": "lib/cli.py", "buggy_sha": "039bcbcded46d24dc0d64512dbece02e44e91584", "fixed_sha": "e48b8a751c439ef153cdfde69aaff5fef5d380f0", "commit_message": "Enhanced error-handling", "changes": 4, "patch": "@@ -4,6 +4,7 @@\n # author = i@cdxy.me\n \n import os.path\n+import traceback\n from lib.parse.cmdline import cmdLineParser\n from lib.core.option import initOptions\n from lib.controller.loader import loadModule, loadPayloads\n@@ -63,6 +64,9 @@ def main():\n     except KeyboardInterrupt:\n         systemQuit(EXIT_STATUS.USER_QUIT)\n \n+    except Exception:\n+        print traceback.format_exc()\n+        logger.warning('It seems like you reached a unhandled exception, please report it to author\\'s mail:<i@cdxy.me> or raise a issue via:<https://github.com/Xyntax/POC-T/issues/new>.')\n \n if __name__ == \"__main__\":\n     main()", "syntax_error": true, "llm_analysis": {"is_bug": true, "is_runtime_error": true, "short_label": "missing_general_exception_handler", "bug_description": "buggy_code 缺少对一般异常的捕获处理，当程序遇到未预期的异常时会直接崩溃退出，用户无法看到详细的错误信息。fixed_code 添加了通用的 Exception 捕获，会打印完整的堆栈跟踪信息并提示用户报告问题，提高了程序的健壮性和用户体验。"}}}
{"id": "bugfix043", "buggy_file": "data/raw_new2/bugfix043_buggy.py", "fixed_file": "data/raw_new2/bugfix043_fixed.py", "meta": {"repo": "Xyntax/POC-T", "file": "plugin/useragent.py", "buggy_sha": "039bcbcded46d24dc0d64512dbece02e44e91584", "fixed_sha": "e48b8a751c439ef153cdfde69aaff5fef5d380f0", "commit_message": "Enhanced error-handling", "changes": 2, "patch": "@@ -40,7 +40,7 @@ def _init_UAlist(path):\n \n \n def get_random_agent(path=paths.UA_LIST_PATH):\n-    if not th.has_key(\"UA_LIST\"):\n+    if \"UA_LIST\" not in th:\n         _init_UAlist(path)\n     try:\n         return random.sample(th.UA_LIST, 1)[0]", "syntax_error": true, "llm_analysis": {"is_bug": true, "is_runtime_error": true, "short_label": "deprecated_dict_has_key_method", "bug_description": "buggy_code 使用了 Python 2 中已弃用的 dict.has_key() 方法，在 Python 3 中会导致 AttributeError 运行时错误。fixed_code 将其替换为更现代且兼容的 'key in dict' 语法。"}}}
{"id": "bugfix044", "buggy_file": "data/raw_new2/bugfix044_buggy.py", "fixed_file": "data/raw_new2/bugfix044_fixed.py", "meta": {"repo": "Xyntax/POC-T", "file": "script/shiro-deserial-rce.py", "buggy_sha": "dfb0232aa1d2d853cd9205250f5d0ea039078bfd", "fixed_sha": "bb0ed1b5ddf0a1d3343da93472cb243be2b376a8", "commit_message": "fix bug in script:shiro-deserial-rce.py", "changes": 2, "patch": "@@ -25,7 +25,7 @@ def poc(url):\n     try:\n         requests.get(target, cookies=cookies, timeout=10)\n         q = queryDnsRecord('shiro.xxx.dnslog.info')\n-        if url.split(':')[0] in q:\n+        if url.split('://')[-1].split(':')[0] in q:\n             return url\n     except Exception, e:\n         pass", "syntax_error": true, "llm_analysis": {"is_bug": true, "is_runtime_error": false, "short_label": "incorrect_url_parsing_for_dns_verification", "bug_description": "Buggy code incorrectly parses URL by splitting on ':' which fails for URLs with protocol (e.g., 'http://example.com:8080'). Fixed code properly extracts hostname by first removing protocol and then handling port."}}}
{"id": "bugfix045", "buggy_file": "data/raw_new2/bugfix045_buggy.py", "fixed_file": "data/raw_new2/bugfix045_fixed.py", "meta": {"repo": "guillaumegenthial/sequence_tagging", "file": "build_data.py", "buggy_sha": "532c4cb0627890390bf56b273a6dc5deed7a0522", "fixed_sha": "acc5e0a984e1bbd8f31b94582e61c9cc6bdd3f06", "commit_message": "python3 and tf1.2 support + bug fix", "changes": 5, "patch": "@@ -1,4 +1,4 @@\n-from config import config\n+from config import Config\n from data_utils import CoNLLDataset, get_vocabs, UNK, NUM, \\\n     get_glove_vocab, write_vocab, load_vocab, get_char_vocab, \\\n     export_trimmed_glove_vectors, get_processing_word\n@@ -14,7 +14,7 @@ def build_data(config):\n         creates vocab files from the datasets\n         creates a npz embedding file from trimmed glove vectors\n     \"\"\"\n-    processing_word = get_processing_word(lowercase=config.lowercase)\n+    processing_word = get_processing_word(lowercase=True)\n \n     # Generators\n     dev   = CoNLLDataset(config.dev_filename, processing_word)\n@@ -45,4 +45,5 @@ def build_data(config):\n \n \n if __name__ == \"__main__\":\n+    config = Config()\n     build_data(config)\n\\ No newline at end of file", "syntax_error": false, "llm_analysis": {"is_bug": true, "is_runtime_error": true, "short_label": "config_object_instantiation_missing", "bug_description": "buggy_code 中存在两个问题：1) 从 config 模块导入的是 config 对象而不是 Config 类，导致无法实例化；2) 在 main 块中直接使用未实例化的 config 对象。这会导致 AttributeError 运行时错误，因为 config 对象不存在。fixed_code 修复了这两个问题：导入 Config 类并在 main 块中正确实例化 config 对象。"}}}
{"id": "bugfix046", "buggy_file": "data/raw_new2/bugfix046_buggy.py", "fixed_file": "data/raw_new2/bugfix046_fixed.py", "meta": {"repo": "guillaumegenthial/sequence_tagging", "file": "config.py", "buggy_sha": "532c4cb0627890390bf56b273a6dc5deed7a0522", "fixed_sha": "acc5e0a984e1bbd8f31b94582e61c9cc6bdd3f06", "commit_message": "python3 and tf1.2 support + bug fix", "changes": 49, "patch": "@@ -1,28 +1,53 @@\n-class config():\n+import os\n+from general_utils import get_logger\n+\n+\n+class Config():\n+    def __init__(self):\n+        # directory for training outputs\n+        if not os.path.exists(self.output_path):\n+            os.makedirs(self.output_path)\n+\n+        # create instance of logger\n+        self.logger = get_logger(self.log_path)\n+        \n+\n+    # general config\n+    output_path = \"results/crf/\"\n+    model_output = output_path + \"model.weights/\"\n+    log_path = output_path + \"log.txt\"\n+\n+    # embeddings\n     dim = 300\n     dim_char = 100\n     glove_filename = \"data/glove.6B/glove.6B.{}d.txt\".format(dim)\n     trimmed_filename = \"data/glove.6B.{}d.trimmed.npz\".format(dim)\n-    words_filename = \"data/words.txt\"\n-    tags_filename = \"data/tags.txt\"\n-    chars_filename = \"data/chars.txt\"\n+\n+    # dataset\n     dev_filename = \"data/coNLL/eng/eng.testa.iob\"\n     test_filename = \"data/coNLL/eng/eng.testb.iob\"\n     train_filename = \"data/coNLL/eng/eng.train.iob\"\n-    max_iter = None\n-    lowercase = True\n+    max_iter = None # if not None, max number of examples\n+\n+    # vocab (created from dataset with build_data.py)\n+    words_filename = \"data/words.txt\"\n+    tags_filename = \"data/tags.txt\"\n+    chars_filename = \"data/chars.txt\"\n+    \n+    # training\n     train_embeddings = False\n-    nepochs = 20\n+    nepochs = 15\n     dropout = 0.5\n     batch_size = 20\n     lr = 0.001\n     lr_decay = 0.9\n     nepoch_no_imprv = 3\n \n+    # model hyperparameters\n     hidden_size = 300\n     char_hidden_size = 100\n-    crf = True # if crf, training is 1.7x slower\n-    chars = True # if char embedding, training is 3.5x slower\n-    output_path = \"results/crf/\"\n-    model_output = output_path + \"model.weights/\"\n-    log_path = output_path + \"log.txt\"\n+    \n+    # NOTE: if both chars and crf, only 1.6x slower on GPU\n+    crf = True # if crf, training is 1.7x slower on CPU\n+    chars = True # if char embedding, training is 3.5x slower on CPU\n+", "syntax_error": false, "llm_analysis": {"is_bug": true, "is_runtime_error": true, "short_label": "missing_directory_creation", "bug_description": "buggy_code 缺少输出目录的自动创建逻辑，当 results/crf/ 目录不存在时，程序在尝试写入模型文件或日志时会抛出 FileNotFoundError 异常。fixed_code 通过添加 __init__ 方法和 os.makedirs() 调用，在初始化时自动创建所需目录，防止了运行时错误。"}}}
{"id": "bugfix047", "buggy_file": "data/raw_new2/bugfix047_buggy.py", "fixed_file": "data/raw_new2/bugfix047_fixed.py", "meta": {"repo": "guillaumegenthial/sequence_tagging", "file": "general_utils.py", "buggy_sha": "532c4cb0627890390bf56b273a6dc5deed7a0522", "fixed_sha": "acc5e0a984e1bbd8f31b94582e61c9cc6bdd3f06", "commit_message": "python3 and tf1.2 support + bug fix", "changes": 7, "patch": "@@ -5,15 +5,16 @@\n \n \n def print_sentence(logger, data):\n-    \"\"\"Adapted from Assignment 3 of CS224N\n+    \"\"\"\n+    Adapted from Assignment 3 of CS224N\n \n     Args:\n         logger: logger instance\n         data: dict d[\"x\"] = [\"I\", \"live\", ...]\n     \"\"\"\n-    spacings = [max([len(seq[i]) for seq in data.itervalues()]) for i in range(len(data[data.keys()[0]]))]\n+    spacings = [max([len(seq[i]) for seq in data.values()]) for i in range(len(data[list(data.keys())[0]]))]\n     # Compute the word spacing\n-    for key, seq in data.iteritems():\n+    for key, seq in data.items():\n         # logger.info(\"{} : \".format(key))\n         to_print = \"\"\n         for token, spacing in zip(seq, spacings):", "syntax_error": false, "llm_analysis": {"is_bug": true, "is_runtime_error": true, "short_label": "python3_compatibility_dict_methods", "bug_description": "buggy_code 使用了 Python 2 的 dict.itervalues() 和 dict.iteritems() 方法，以及 data.keys()[0] 的索引方式，这些在 Python 3 中会导致 AttributeError 或 TypeError。fixed_code 将其改为 Python 3 兼容的 data.values()、data.items() 和 list(data.keys())[0]。"}}}
{"id": "bugfix048", "buggy_file": "data/raw_new2/bugfix048_buggy.py", "fixed_file": "data/raw_new2/bugfix048_fixed.py", "meta": {"repo": "owocki/pytrader", "file": "history/predict.py", "buggy_sha": "e576ec9cb14b8ea488a29dcab51e74073625d9f7", "fixed_sha": "347efbbb4d1bde969c3f3105d2ad751cb584e2dc", "commit_message": "Merge pull request #73 from igorpejic/master\n\nTravis CI status readme badge + predict_many_v2 bugfix", "changes": 5, "patch": "@@ -11,8 +11,8 @@ def predict_v2(ticker, hidden_layers=15, NUM_MINUTES_BACK=1000, NUM_EPOCHS=1000,\n     # setup\n     print_and_log(\"(p)starting ticker:{} hidden:{} min:{} epoch:{} gran:{} dsinputs:{} learningrate:{} bias:{} momentum:{} weightdecay:{}\\\n                   recurrent:{}, timedelta_back_in_granularity_increments:{} \".format(\n-                      ticker, hidden_layers, NUM_MINUTES_BACK, NUM_EPOCHS, granularity_minutes, datasetinputs,\n-                      learningrate, bias, momentum, weightdecay, recurrent, timedelta_back_in_granularity_increments))\n+                  ticker, hidden_layers, NUM_MINUTES_BACK, NUM_EPOCHS, granularity_minutes, datasetinputs,\n+                  learningrate, bias, momentum, weightdecay, recurrent, timedelta_back_in_granularity_increments))\n     pt = PredictionTest()\n     pt.type = 'mock'\n     pt.symbol = ticker\n@@ -44,6 +44,7 @@ def predict_v2(ticker, hidden_layers=15, NUM_MINUTES_BACK=1000, NUM_EPOCHS=1000,\n     for i, val in enumerate(test_data):\n         try:\n             # get NN projection\n+            pt.get_nn()\n             sample = create_sample_row(test_data, i, datasetinputs)\n             recommend, nn_price, last_sample, projected_change_pct = pt.predict(sample)\n ", "syntax_error": false, "llm_analysis": {"is_bug": true, "is_runtime_error": true, "short_label": "missing_neural_network_initialization", "bug_description": "buggy_code 在循环中调用 pt.predict(sample) 之前没有初始化神经网络，这会导致 AttributeError 或其他运行时错误，因为神经网络对象不存在。fixed_code 通过在每次预测前添加 pt.get_nn() 来正确初始化神经网络。"}}}
{"id": "bugfix049", "buggy_file": "data/raw_new2/bugfix049_buggy.py", "fixed_file": "data/raw_new2/bugfix049_fixed.py", "meta": {"repo": "openai/pixel-cnn", "file": "data/imagenet_data.py", "buggy_sha": "7db97c085600e2fd9c48b5c999444c55715d8fa7", "fixed_sha": "2010d6d286878481ad1c59ef8784acbefa76dc2a", "commit_message": "fix param init bug in new TF versions", "changes": 2, "patch": "@@ -87,7 +87,7 @@ def load(data_dir, subset='train'):\n class DataLoader(object):\n     \"\"\" an object that generates batches of CIFAR-10 data for training \"\"\"\n \n-    def __init__(self, data_dir, subset, batch_size, rng=None, shuffle=False):\n+    def __init__(self, data_dir, subset, batch_size, rng=None, shuffle=False, **kwargs):\n         \"\"\" \n         - data_dir is location where the files are stored\n         - subset is train|test ", "syntax_error": false, "llm_analysis": {"is_bug": true, "is_runtime_error": true, "short_label": "missing_kwargs_in_init", "bug_description": "在buggy_code中，DataLoader的__init__方法缺少**kwargs参数，当传递额外的关键字参数时会引发TypeError异常。fixed_code通过添加**kwargs参数来接受并忽略额外的关键字参数，避免了运行时错误。"}}}
{"id": "bugfix050", "buggy_file": "data/raw_new2/bugfix050_buggy.py", "fixed_file": "data/raw_new2/bugfix050_fixed.py", "meta": {"repo": "openai/pixel-cnn", "file": "train.py", "buggy_sha": "0ff8ab51a9685bdb47ea7823ffeaee1c4f2009c6", "fixed_sha": "d6f19ad9ccf4513459442af21df463f10a79642f", "commit_message": "fix", "changes": 2, "patch": "@@ -67,7 +67,7 @@\n \n # data place holders\n x_init = tf.placeholder(tf.float32, shape=(args.init_batch_size,) + obs_shape)\n-xs = [tf.placeholder(tf.float32, shape=(args.batch_size, ) + obs_shape) for i in range(args.batch_size)]\n+xs = [tf.placeholder(tf.float32, shape=(args.batch_size, ) + obs_shape) for i in range(args.nr_gpu)]\n \n # if the model is class-conditional we'll set up label placeholders + one-hot encodings 'h' to condition on\n if args.class_conditional:", "syntax_error": false, "llm_analysis": {"is_bug": true, "is_runtime_error": true, "short_label": "incorrect_gpu_placeholder_count", "bug_description": "在buggy_code中，xs列表的创建使用了错误的循环范围：range(args.batch_size)而不是range(args.nr_gpu)。这会导致当GPU数量不等于batch_size时，程序会因TensorFlow占位符数量不匹配而崩溃。fixed版本将循环范围修正为range(args.nr_gpu)，确保为每个GPU创建正确的占位符数量。"}}}
