# LLT Assistant Backend

FastAPI backend service for analyzing Python pytest unit tests for defects and redundancies using a hybrid approach (rule engine + LLM).

## Features

- **Hybrid Analysis**: Combines fast rule-based analysis with intelligent LLM analysis
- **Multiple Analysis Modes**:
  - `rules-only`: Fast, deterministic analysis using AST parsing
  - `llm-only`: Deep analysis using AI for complex issues
  - `hybrid`: Best of both worlds - rules for common issues, LLM for complex cases
- **Comprehensive Issue Detection**:
  - Redundant assertions
  - Missing assertions
  - Trivial assertions (always true)
  - Unused fixtures and variables
  - Test code smells (timing dependencies, over-mocking, etc.)
  - Test mergeability analysis
- **Actionable Fix Suggestions**: Provides specific code changes to fix detected issues
- **Async Task Management**: Redis-backed task system for long-running operations
- **Feature 1 - Test Generation**: Generate pytest tests from code and user descriptions
- **Production Ready**: Docker support with Redis, structured logging, error handling

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FastAPI Backend                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   API Layer  â”‚â”€â”€â”€â–¶â”‚ Analysis     â”‚â”€â”€â”€â–¶â”‚ LLM      â”‚ â”‚
â”‚  â”‚   (Routes)   â”‚    â”‚ Orchestrator â”‚    â”‚ Client   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚       â”‚                       â”‚                           â”‚
â”‚       â”‚                       â–¼                           â”‚
â”‚       â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚       â”‚              â”‚  Rule Engine    â”‚                 â”‚
â”‚       â”‚              â”‚  (AST Analysis) â”‚                 â”‚
â”‚       â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚       â”‚                                                  â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                  â”‚
â”‚                      â–¼                                  â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚              â”‚ Task Manager â”‚                           â”‚
â”‚              â”‚   (Redis)    â”‚                           â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Redis Store   â”‚
                    â”‚  (Task State)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Development Status

### âœ… Feature-Complete Implementation

**The LLT Assistant Backend is now fully implemented!** All core components are complete and functional:

#### ğŸ—ï¸ Complete Architecture
- **2,578 lines of application code** across multiple modules
- **Hybrid analysis pipeline** with rule engine + LLM integration
- **Production-ready FastAPI service** with comprehensive endpoints

#### ğŸš€ Implemented Components

**Core Analysis Engine:**
- **AST Parser** (`app/analyzers/ast_parser.py`): Complete Python AST parsing for pytest files
- **Rule Engine** (`app/analyzers/rule_engine.py`): 5 core detection rules implemented
- **LLM Client** (`app/core/llm_client.py`): Async client with retry logic and error handling
- **LLM Analyzer** (`app/core/llm_analyzer.py`): Prompt templates and response parsing
- **Main Analyzer** (`app/core/analyzer.py`): Hybrid orchestration of all analysis modes

**API & Production:**
- **API Routes** (`app/api/v1/routes.py`): Complete `/analyze`, `/health`, `/modes` endpoints
- **Suggestion Generator** (`app/core/suggestion_generator.py`): Actionable fix recommendations
- **Logging** (`app/core/logging_config.py`): Structured JSON logging
- **Docker**: Multi-stage production-ready containerization

#### ğŸ¯ Analysis Modes Supported
- **`rules-only`**: Fast deterministic analysis (~1 second)
- **`llm-only`**: Deep AI analysis (~5-10 seconds)
- **`hybrid`**: Combined optimal approach (~10-15 seconds)

#### ğŸ“Š Issue Detection Capabilities
- Redundant assertions
- Missing assertions
- Trivial assertions (always true)
- Unused fixtures and variables
- Test mergeability opportunities
- Code smells and anti-patterns

### ğŸ”§ Current Status: Production Ready

The backend is feature-complete and ready for production use. The system can:
- Analyze pytest files for quality issues
- Provide actionable fix suggestions with code snippets
- Handle all three analysis modes seamlessly
- Scale horizontally with Docker deployment

**Note**: Minor configuration testing remains, but core functionality is fully operational.

## Quick Start

### Prerequisites

- Python 3.11+
- UV package manager
- Docker and Docker Compose (for containerized deployment)

### Local Development

1. **Install dependencies using UV:**
   ```bash
   uv pip install -e .
   ```

2. **Set up environment variables:**
   ```bash
   cp .env.example .env
   # Edit .env to add your LLM_API_KEY
   ```

3. **Run the development server:**
   ```bash
   uvicorn app.main:app --reload
   ```

4. **Access the API:**
   - API documentation: http://localhost:8886/docs
   - Health check: http://localhost:8886/health

### Docker Deployment

1. **Build and run with Docker Compose:**
   ```bash
   docker-compose up --build
   ```

2. **Services included:**
   - **API Service**: FastAPI backend on port 8886
   - **Redis Service**: Task management and caching on port 6379

3. **The API will be available at:** http://localhost:8886
   - API documentation: http://localhost:8886/docs
   - Health check: http://localhost:8886/health

## API Usage

### Analyze Test Files

**Endpoint:** `POST /quality/analyze`

**Request:**
```json
{
  "files": [
    {
      "path": "test_example.py",
      "content": "def test_example():\n    assert True\n    assert True  # Redundant!\n"
    }
  ],
  "mode": "hybrid"
}
```

**Response:**
```json
{
  "analysis_id": "uuid-123",
  "issues": [
    {
      "file": "test_example.py",
      "line": 3,
      "column": 4,
      "severity": "warning",
      "type": "redundant-assertion",
      "message": "Duplicate assertion found",
      "detected_by": "rule_engine",
      "suggestion": {
        "action": "remove",
        "old_code": "    assert True  # Redundant!",
        "new_code": null,
        "explanation": "Remove this duplicate assertion to reduce redundancy."
      }
    }
  ],
  "metrics": {
    "total_tests": 1,
    "issues_count": 1,
    "analysis_time_ms": 150
  }
}
```

### Available Analysis Modes

- **`rules-only`**: Fast analysis using deterministic rules (AST parsing)
- **`llm-only`**: Deep analysis using AI for complex issues
- **`hybrid`**: Combines both approaches for optimal results

### Get Analysis Modes

**Endpoint:** `GET /modes`

Returns available analysis modes with descriptions.

## Configuration

Configuration is managed through environment variables:

| Variable | Description | Default |
|----------|-------------|---------|
| `LLM_API_KEY` | API key for LLM service | Required |
| `LLM_BASE_URL` | LLM API base URL | `https://api.qnaigc.com/v1` |
| `LLM_MODEL` | LLM model to use | `deepseek/deepseek-v3.2-exp` |
| `LOG_LEVEL` | Logging level | `INFO` |
| `LOG_FORMAT` | Log format (json/text) | `json` |
| `MAX_FILE_SIZE` | Maximum file size in bytes | `1048576` (1MB) |
| `MAX_FILES_PER_REQUEST` | Maximum files per request | `50` |
| `REDIS_URL` | Redis connection URL for task management | `redis://localhost:6379/0` |

### Redis Configuration

The application uses Redis for async task management (Feature 1 - Test Generation).

**Local Development:**
- Default: `redis://localhost:6379/0`
- Ensure Redis is running locally or use Docker Compose

**Docker Compose:**
- Automatically configured: `redis://redis:6379/0`
- Redis service is included in `docker-compose.yml`
- Data is persisted in a Docker volume (`redis-data`)

**Production:**
- Set `REDIS_URL` to your Redis instance (e.g., `redis://redis.example.com:6379/0`)
- Supports SSL connections: `rediss://` (with SSL certificate validation)

## Development

### Project Structure

```
llt-backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py              # FastAPI application entry
â”‚   â”œâ”€â”€ config.py            # Configuration management
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ v1/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ routes.py    # API endpoints
â”‚   â”‚       â””â”€â”€ schemas.py   # Pydantic models
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ analyzer.py      # Main analysis orchestrator
â”‚   â”‚   â”œâ”€â”€ llm_client.py    # LLM API client
â”‚   â”‚   â”œâ”€â”€ llm_analyzer.py  # LLM prompt templates
â”‚   â”‚   â”œâ”€â”€ suggestion_generator.py  # Fix suggestions
â”‚   â”‚   â””â”€â”€ logging_config.py # Structured logging
â”‚   â”œâ”€â”€ analyzers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ rule_engine.py   # Static analysis rules
â”‚   â”‚   â””â”€â”€ ast_parser.py    # Python AST parsing
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ issue.py         # Issue data models
â”œâ”€â”€ tests/                   # Test suite
â”œâ”€â”€ Dockerfile              # Container definition
â”œâ”€â”€ docker-compose.yml      # Service orchestration
â”œâ”€â”€ pyproject.toml          # Project dependencies
â””â”€â”€ README.md              # This file
```

### Running Tests

```bash
# Install dev dependencies
uv pip install -e ".[dev]"

# Run tests
pytest tests/

# Run with coverage
pytest tests/ --cov=app --cov-report=html
```

### Code Quality

The project uses several tools for code quality:

- **Black**: Code formatting
- **isort**: Import sorting
- **mypy**: Type checking
- **pytest**: Testing framework

```bash
# Format code
black app/ tests/

# Sort imports
isort app/ tests/

# Type check
mypy app/

# Run tests
pytest
```

## Issue Detection Rules

### Rule Engine (Fast, Deterministic)

1. **Redundant Assertions**: Duplicate assertions within the same test
2. **Missing Assertions**: Test functions with no assertions
3. **Trivial Assertions**: Assertions that always pass (e.g., `assert True`)
4. **Unused Fixtures**: Fixtures defined but never used
5. **Unused Variables**: Variables assigned but never referenced

### LLM Analysis (Intelligent, Contextual)

1. **Test Mergeability**: Identifies tests that could be logically merged
2. **Assertion Quality**: Evaluates if assertions are sufficient and meaningful
3. **Test Smells**: Detects code smells like timing dependencies, over-mocking, etc.

## Deployment

### Cloud Platforms

The application is designed for easy deployment on:
- **Heroku**: Use the Dockerfile for container deployment
- **DigitalOcean**: Use Docker Compose for multi-service setup
- **Fly.io**: Deploy using the provided Dockerfile

### Production Considerations

1. **Security**: Configure CORS appropriately for your domain
2. **Scaling**: The application is stateless and can be horizontally scaled
3. **Monitoring**: Structured JSON logs for easy parsing
4. **Health Checks**: `/health` endpoint for load balancer health checks
5. **Rate Limiting**: Implement rate limiting for the `/analyze` endpoint

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests for new functionality
5. Run the test suite
6. Submit a pull request

## License

MIT License - see LICENSE file for details.

## Support

For issues and questions:
1. Check the documentation
2. Search existing issues
3. Create a new issue with detailed information

---

**Note**: This project requires an LLM API key for the AI-powered analysis features. The rule-based analysis works without API keys.
